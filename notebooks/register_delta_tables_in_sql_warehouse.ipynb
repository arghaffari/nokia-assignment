{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527bdfaa-3c23-44ff-86d2-a430d998b8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `nokia-assginment-catalog`;\n",
    "-- drop schema patent_data cascade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf7858b3-4bcc-4dcd-86bd-be82221cbe7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to create a widget to control schema dropping\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"drop_patent_data_schema\", \"false\", [\"true\", \"false\"], \"Drop schema patent_data cascade\")\n",
    "    drop_patent_data_schema = dbutils.widgets.get(\"drop_patent_data_schema\") == \"true\"\n",
    "except:\n",
    "    # Default to not dropping schema in job mode\n",
    "    drop_patent_data_schema = False\n",
    "\n",
    "print(f\"Drop patent_data schema setting: {drop_patent_data_schema}\")\n",
    "\n",
    "# Execute SQL to drop schema if requested\n",
    "if drop_patent_data_schema:\n",
    "    try:\n",
    "        print(\"Dropping schema patent_data cascade...\")\n",
    "        spark.sql(\"DROP SCHEMA IF EXISTS patent_data CASCADE\")\n",
    "        print(\"Schema patent_data successfully dropped\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping schema: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eda366a-0657-470e-a711-03008a94bb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "import traceback\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session with Delta Lake support\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Patent Delta Tables Registration\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def check_path_exists(path):\n",
    "    \"\"\"Check if a path exists and is accessible\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_delta_table(spark, path):\n",
    "    \"\"\"Check if a path is a valid Delta table\"\"\"\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_new_data_processed():\n",
    "    \"\"\"\n",
    "    Check if new data was processed in the previous gold layer step\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether new data was processed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Method 1: Check processing metadata table\n",
    "        spark = initialize_spark()\n",
    "        \n",
    "        # Check if the processing_metadata table exists\n",
    "        database_name = \"patent_data\"\n",
    "        try:\n",
    "            metadata_df = spark.sql(f\"SELECT * FROM {database_name}.processing_metadata ORDER BY processing_timestamp DESC LIMIT 1\")\n",
    "            if metadata_df.count() > 0:\n",
    "                last_record = metadata_df.first()\n",
    "                new_data_processed = last_record.new_data_processed\n",
    "                print(f\"Found processing metadata: new_data_processed={new_data_processed}\")\n",
    "                return new_data_processed\n",
    "        except:\n",
    "            print(\"Could not query processing_metadata table\")\n",
    "        \n",
    "        # Method 2: Check processing status Delta file\n",
    "        try:\n",
    "            processing_status_path = \"/Volumes/nokia-assginment-catalog/processing_status/status\"\n",
    "            if check_path_exists(processing_status_path):\n",
    "                status_df = spark.read.format(\"delta\").load(processing_status_path)\n",
    "                if status_df.count() > 0:\n",
    "                    status = status_df.first().new_data_processed\n",
    "                    print(f\"Found processing status file: new_data_processed={status}\")\n",
    "                    return status\n",
    "        except:\n",
    "            print(\"Could not read processing status file\")\n",
    "        \n",
    "        # Method 3: Check for previous notebook result\n",
    "        try:\n",
    "            # Try to get result from previous notebook\n",
    "            prev_result_str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().parentContext().get(\"result\")\n",
    "            if prev_result_str:\n",
    "                prev_result = json.loads(prev_result_str)\n",
    "                if \"new_data_processed\" in prev_result:\n",
    "                    status = prev_result[\"new_data_processed\"]\n",
    "                    print(f\"Found previous notebook result: new_data_processed={status}\")\n",
    "                    return status\n",
    "        except:\n",
    "            print(\"Could not get previous notebook result\")\n",
    "        \n",
    "        # Default: If we can't determine, assume there might be new data\n",
    "        print(\"Could not determine if new data was processed, assuming yes\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking for new data: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        # If there's an error, assume there might be new data\n",
    "        return True\n",
    "\n",
    "def register_and_upsert_table(spark, delta_path, table_name, database_name=\"patent_data\"):\n",
    "    \"\"\"\n",
    "    Register a Delta table if it doesn't exist and upsert data\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        delta_path: Path to Delta table\n",
    "        table_name: Name of the table to create/update\n",
    "        database_name: Database to create the table in\n",
    "    \n",
    "    Returns:\n",
    "        dict: Operation result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Debug - what's in the gold delta path?\n",
    "        print(f\"Contents of {delta_path}:\")\n",
    "        path_files = dbutils.fs.ls(delta_path)\n",
    "        for file in path_files[:5]:  # Show first 5 items\n",
    "            print(f\"  - {file.name} ({file.size} bytes)\")\n",
    "        \n",
    "        # Debug - how many batches are in silver layer\n",
    "        try:\n",
    "            silver_path = \"/Volumes/nokia-assginment-catalog/silver/patent_data\"\n",
    "            silver_dirs = [d for d in dbutils.fs.ls(silver_path) if d.isDir() and not d.name.startswith('_')]\n",
    "            print(f\"Silver layer has {len(silver_dirs)} batch directories\")\n",
    "        except:\n",
    "            print(\"Could not access silver layer to count batches\")\n",
    "        \n",
    "        # Debug - how many batches have checkpoints\n",
    "        try:\n",
    "            checkpoint_location = \"/Volumes/nokia-assginment-catalog/checkpoints/checkpoints_data/gold_autoloader/\"\n",
    "            if check_path_exists(checkpoint_location):\n",
    "                checkpoints = [d for d in dbutils.fs.ls(checkpoint_location) if not d.name.startswith('_')]\n",
    "                print(f\"Found {len(checkpoints)} checkpoint files\")\n",
    "        except:\n",
    "            print(\"Could not check checkpoint files\")\n",
    "\n",
    "        # Create database if it doesn't exist\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        \n",
    "        # Fully qualified table name\n",
    "        full_table_name = f\"{database_name}.{table_name}\"\n",
    "        \n",
    "        # Read the Delta data (source)\n",
    "        source_df = spark.read.format(\"delta\").load(delta_path)\n",
    "        \n",
    "        # Check if source has data and print more details for debugging\n",
    "        source_count = source_df.count()\n",
    "        print(f\"Source Delta file at {delta_path} has {source_count} records\")\n",
    "        \n",
    "        if source_count == 0:\n",
    "            return {\n",
    "                \"status\": \"warning\",\n",
    "                \"message\": f\"Source Delta file at {delta_path} is empty\"\n",
    "            }\n",
    "            \n",
    "        # Check if table already exists\n",
    "        table_exists = False\n",
    "        existing_tables = [t.name for t in spark.catalog.listTables(database_name)]\n",
    "        if table_name in existing_tables:\n",
    "            table_exists = True\n",
    "            \n",
    "        # For individual tables, use overwrite mode instead of merge\n",
    "        is_individual_table = \"_individual\" in table_name\n",
    "        \n",
    "        if not table_exists:\n",
    "            # Create table if it doesn't exist\n",
    "            print(f\"Creating table {full_table_name} from {delta_path}\")\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            \n",
    "            # Configure table properties for optimization\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {full_table_name}\n",
    "                SET TBLPROPERTIES (\n",
    "                    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "                    'delta.autoOptimize.autoCompact' = 'true',\n",
    "                    'delta.enableChangeDataFeed' = 'true',\n",
    "                    'delta.logRetentionDuration' = '30 days',\n",
    "                    'delta.tuneFileSizesForRewrites' = 'true'\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"Table {full_table_name} created successfully with {source_count} rows\")\n",
    "            return {\n",
    "                \"status\": \"created\",\n",
    "                \"message\": f\"Table {full_table_name} created with {source_count} rows\"\n",
    "            }\n",
    "        elif is_individual_table:\n",
    "            # For individual tables, keep a record of how many rows we're about to replace\n",
    "            target_df = spark.read.table(full_table_name)\n",
    "            target_count = target_df.count()\n",
    "            print(f\"Replacing {target_count} existing rows with {source_count} rows in {full_table_name}\")\n",
    "            \n",
    "            # If source has fewer records than target, this could indicate a problem\n",
    "            if source_count < target_count and target_count > 0:\n",
    "                print(f\"WARNING: Source has fewer records ({source_count}) than existing table ({target_count})\")\n",
    "            \n",
    "            # For individual tables, use complete replacement instead of merge\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            print(f\"Data in {full_table_name} replaced successfully with {source_count} rows\")\n",
    "            return {\n",
    "                \"status\": \"replaced\",\n",
    "                \"message\": f\"Table {full_table_name} completely replaced with {source_count} rows (previous: {target_count})\"\n",
    "            }\n",
    "        else:\n",
    "            # For regular tables, perform upsert\n",
    "            if \"publication_number\" not in source_df.columns:\n",
    "                return {\n",
    "                    \"status\": \"warning\",\n",
    "                    \"message\": f\"Source data doesn't have publication_number column\"\n",
    "                }\n",
    "            \n",
    "            # Get target table and count before merge\n",
    "            target_table = DeltaTable.forName(spark, full_table_name)\n",
    "            target_count_before = spark.read.table(full_table_name).count()\n",
    "            \n",
    "            print(f\"Performing upsert to {full_table_name} from {delta_path}\")\n",
    "            print(f\"Target table has {target_count_before} rows before merge\")\n",
    "            \n",
    "            # Sample a few publication numbers from source for debugging\n",
    "            sample_pubs = source_df.select(\"publication_number\").limit(5).collect()\n",
    "            print(f\"Sample publication numbers from source: {[row.publication_number for row in sample_pubs]}\")\n",
    "            \n",
    "            # Perform merge operation\n",
    "            target_table.alias(\"target\").merge(\n",
    "                source_df.alias(\"source\"),\n",
    "                \"target.publication_number = source.publication_number\"\n",
    "            ).whenMatchedUpdateAll(\n",
    "            ).whenNotMatchedInsertAll(\n",
    "            ).execute()\n",
    "            \n",
    "            # Count after merge to see if anything changed\n",
    "            target_count_after = spark.read.table(full_table_name).count()\n",
    "            rows_changed = target_count_after - target_count_before\n",
    "            \n",
    "            print(f\"Upsert to {full_table_name} completed. Rows before: {target_count_before}, after: {target_count_after}\")\n",
    "            print(f\"Net change in rows: {rows_changed}\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"updated\",\n",
    "                \"message\": f\"Table {full_table_name} updated with upsert. Net new rows: {rows_changed}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing {table_name}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": error_message\n",
    "        }\n",
    "\n",
    "def register_and_upsert_all_tables(gold_base_path=\"/Volumes/nokia-assginment-catalog/gold\", \n",
    "                                   database_name=\"patent_data\"):\n",
    "    \"\"\"\n",
    "    Process all patent gold tables - register if needed and upsert data\n",
    "    \n",
    "    Args:\n",
    "        gold_base_path: Base path to gold Delta tables\n",
    "        database_name: Database to create tables in\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results for each table\n",
    "    \"\"\"\n",
    "    spark = initialize_spark()\n",
    "    results = {}\n",
    "    \n",
    "    # Create control table if it doesn't exist\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {database_name}.patent_update_control (\n",
    "            update_id STRING,\n",
    "            update_timestamp TIMESTAMP,\n",
    "            result STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create control table: {str(e)}\")\n",
    "    \n",
    "    # Define tables to process\n",
    "    tables = [\n",
    "        {\"path\": f\"{gold_base_path}/patents\", \"name\": \"patents\"},\n",
    "        {\"path\": f\"{gold_base_path}/ipc_classifications\", \"name\": \"ipc_classifications\"},\n",
    "        {\"path\": f\"{gold_base_path}/ipc_individual\", \"name\": \"ipc_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/inventors\", \"name\": \"inventors\"},\n",
    "        {\"path\": f\"{gold_base_path}/inventors_individual\", \"name\": \"inventors_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/applicants\", \"name\": \"applicants\"},\n",
    "        {\"path\": f\"{gold_base_path}/applicants_individual\", \"name\": \"applicants_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/us_applicants\", \"name\": \"us_applicants\"},\n",
    "        {\"path\": f\"{gold_base_path}/us_applicants_individual\", \"name\": \"us_applicants_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/claims\", \"name\": \"claims\"},\n",
    "        {\"path\": f\"{gold_base_path}/claims_individual\", \"name\": \"claims_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/complete_patents\", \"name\": \"complete_patents\"}\n",
    "    ]\n",
    "    \n",
    "    # Register and upsert each table\n",
    "    for table_info in tables:\n",
    "        delta_path = table_info[\"path\"]\n",
    "        table_name = table_info[\"name\"]\n",
    "        \n",
    "        # Check if Delta path exists\n",
    "        if not check_path_exists(delta_path):\n",
    "            print(f\"Delta path does not exist: {delta_path}\")\n",
    "            results[table_name] = {\"status\": \"skipped\", \"message\": \"Delta path not found\"}\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a valid Delta table\n",
    "        if not is_delta_table(spark, delta_path):\n",
    "            print(f\"Path is not a valid Delta table: {delta_path}\")\n",
    "            results[table_name] = {\"status\": \"skipped\", \"message\": \"Not a valid Delta table\"}\n",
    "            continue\n",
    "            \n",
    "        # Register and upsert the table\n",
    "        result = register_and_upsert_table(\n",
    "            spark, \n",
    "            delta_path, \n",
    "            table_name, \n",
    "            database_name=database_name\n",
    "        )\n",
    "        \n",
    "        results[table_name] = result\n",
    "    \n",
    "    # Log this batch of updates in the control table\n",
    "    try:\n",
    "        result_json = json.dumps(results)\n",
    "        update_id = str(uuid.uuid4())\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO {database_name}.patent_update_control\n",
    "        VALUES (\n",
    "            '{update_id}',\n",
    "            current_timestamp(),\n",
    "            '{result_json}'\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"Update logged in control table with ID: {update_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not log update in control table: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function that runs as part of the workflow after gold process notebook\"\"\"\n",
    "    \n",
    "    # Check if we should run based on whether new data was processed in the previous step\n",
    "    try:\n",
    "        dbutils.widgets.dropdown(\"force_update\", \"false\", [\"true\", \"false\"], \"Force Table Update\")\n",
    "        force_update = dbutils.widgets.get(\"force_update\") == \"true\"\n",
    "    except:\n",
    "        force_update = False\n",
    "    \n",
    "    # Check if new data was processed in the previous step\n",
    "    new_data_processed = check_new_data_processed()\n",
    "    \n",
    "    # Skip processing if no new data and not forcing update\n",
    "    if not new_data_processed and not force_update:\n",
    "        print(\"No new data was processed and force_update=false, skipping table registration\")\n",
    "        result = {\n",
    "            \"status\": \"skipped\",\n",
    "            \"message\": \"No new data was processed and force_update=false\"\n",
    "        }\n",
    "        dbutils.notebook.exit(json.dumps(result))\n",
    "        return\n",
    "    \n",
    "    # Base path to gold Delta tables\n",
    "    gold_base_path = \"/Volumes/nokia-assginment-catalog/gold\"\n",
    "    database_name = \"patent_data\"\n",
    "    \n",
    "    print(f\"Starting patent data registration and upsert process\")\n",
    "    if new_data_processed:\n",
    "        print(\"New data was processed in previous step\")\n",
    "    if force_update:\n",
    "        print(\"Force update requested\")\n",
    "        \n",
    "    print(f\"Source: {gold_base_path}\")\n",
    "    print(f\"Target database: {database_name}\")\n",
    "    \n",
    "    # Register and upsert all tables\n",
    "    results = register_and_upsert_all_tables(\n",
    "        gold_base_path=gold_base_path,\n",
    "        database_name=database_name\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    created_count = sum(1 for r in results.values() if r.get('status') == 'created')\n",
    "    updated_count = sum(1 for r in results.values() if r.get('status') == 'updated')\n",
    "    replaced_count = sum(1 for r in results.values() if r.get('status') == 'replaced')\n",
    "    error_count = sum(1 for r in results.values() if r.get('status') == 'error')\n",
    "    skipped_count = sum(1 for r in results.values() if r.get('status') == 'skipped')\n",
    "    warning_count = sum(1 for r in results.values() if r.get('status') == 'warning')\n",
    "    \n",
    "    print(\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Tables created: {created_count}\")\n",
    "    print(f\"Tables updated via upsert: {updated_count}\")\n",
    "    print(f\"Tables completely replaced: {replaced_count}\")\n",
    "    print(f\"Tables with errors: {error_count}\")\n",
    "    print(f\"Tables skipped: {skipped_count}\")\n",
    "    print(f\"Tables with warnings: {warning_count}\")\n",
    "    \n",
    "    # Print details of any errors\n",
    "    if error_count > 0:\n",
    "        print(\"\\n=== ERROR DETAILS ===\")\n",
    "        for table, result in results.items():\n",
    "            if result.get('status') == 'error':\n",
    "                print(f\"{table}: {result.get('message')}\")\n",
    "    \n",
    "    print(\"\\nProcess completed\")\n",
    "    \n",
    "    # Return results as JSON\n",
    "    final_result = {\n",
    "        \"status\": \"completed\",\n",
    "        \"created\": created_count,\n",
    "        \"updated\": updated_count,\n",
    "        \"replaced\": replaced_count,\n",
    "        \"errors\": error_count,\n",
    "        \"skipped\": skipped_count,\n",
    "        \"warnings\": warning_count\n",
    "    }\n",
    "    dbutils.notebook.exit(json.dumps(final_result))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3311438205404710,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "register_delta_tables_in_sql_warehouse",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
