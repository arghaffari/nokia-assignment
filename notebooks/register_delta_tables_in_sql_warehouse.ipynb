{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527bdfaa-3c23-44ff-86d2-a430d998b8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `nokia-assginment-catalog`;\n",
    "-- drop schema patent_data cascade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1fe0fa-e8f3-41de-a851-482f08ee2820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from delta.tables import DeltaTable\n",
    "# import os\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# import traceback\n",
    "# import json\n",
    "\n",
    "# def initialize_spark():\n",
    "#     \"\"\"Initialize Spark session with Delta Lake support\"\"\"\n",
    "#     return SparkSession.builder \\\n",
    "#         .appName(\"Patent Delta Tables Registration\") \\\n",
    "#         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "# def check_path_exists(path):\n",
    "#     \"\"\"Check if a path exists and is accessible\"\"\"\n",
    "#     try:\n",
    "#         dbutils.fs.ls(path)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# def is_delta_table(spark, path):\n",
    "#     \"\"\"Check if a path is a valid Delta table\"\"\"\n",
    "#     try:\n",
    "#         DeltaTable.forPath(spark, path)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# def register_delta_table(spark, delta_path, table_name, database_name=\"patent_data\", drop_if_exists=False):\n",
    "#     \"\"\"\n",
    "#     Register a Delta table in the Databricks metastore\n",
    "    \n",
    "#     Args:\n",
    "#         spark: SparkSession\n",
    "#         delta_path: Path to Delta table\n",
    "#         table_name: Name of the table to create\n",
    "#         database_name: Database to create the table in\n",
    "#         drop_if_exists: Whether to drop the table if it already exists\n",
    "    \n",
    "#     Returns:\n",
    "#         bool: Success status\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Create database if it doesn't exist\n",
    "#         spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        \n",
    "#         # Fully qualified table name\n",
    "#         full_table_name = f\"{database_name}.{table_name}\"\n",
    "        \n",
    "#         # Check if table already exists\n",
    "#         table_exists = False\n",
    "#         existing_tables = [t.name for t in spark.catalog.listTables(database_name)]\n",
    "#         if table_name in existing_tables:\n",
    "#             table_exists = True\n",
    "            \n",
    "#         # Drop table if requested and it exists\n",
    "#         if table_exists and drop_if_exists:\n",
    "#             print(f\"Dropping existing table {full_table_name}\")\n",
    "#             spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "#             table_exists = False\n",
    "            \n",
    "#         # Register Delta table\n",
    "#         if not table_exists:\n",
    "#             print(f\"Creating table {full_table_name} from {delta_path}\")\n",
    "            \n",
    "#             # Read the Delta table to get the data\n",
    "#             delta_df = spark.read.format(\"delta\").load(delta_path)\n",
    "            \n",
    "#             # Write directly to a table in the patent_data schema\n",
    "#             delta_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            \n",
    "#             # Configure table properties for optimization\n",
    "#             spark.sql(f\"\"\"\n",
    "#                 ALTER TABLE {full_table_name}\n",
    "#                 SET TBLPROPERTIES (\n",
    "#                     'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "#                     'delta.autoOptimize.autoCompact' = 'true',\n",
    "#                     'delta.enableChangeDataFeed' = 'true',\n",
    "#                     'delta.logRetentionDuration' = '30 days',\n",
    "#                     'delta.tuneFileSizesForRewrites' = 'true'\n",
    "#                 )\n",
    "#             \"\"\")\n",
    "                \n",
    "#             print(f\"Table {full_table_name} created with optimization properties\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(f\"Table {full_table_name} already exists\")\n",
    "#             return True\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error registering Delta table {table_name}: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return False\n",
    "\n",
    "# def perform_upsert(spark, table_name, source_path, database_name=\"patent_data\"):\n",
    "#     \"\"\"\n",
    "#     Perform upsert operation (merge) on a Delta table using PySpark\n",
    "    \n",
    "#     Args:\n",
    "#         spark: SparkSession\n",
    "#         table_name: Name of the table to update\n",
    "#         source_path: Path to source Delta table with new data\n",
    "#         database_name: Database containing the table\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Result with status and message\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Fully qualified table name\n",
    "#         full_table_name = f\"{database_name}.{table_name}\"\n",
    "        \n",
    "#         # Read the new data\n",
    "#         new_data_df = spark.read.format(\"delta\").load(source_path)\n",
    "        \n",
    "#         # Check if publication_number exists in the source data\n",
    "#         if \"publication_number\" not in new_data_df.columns:\n",
    "#             return {\n",
    "#                 \"status\": \"error\", \n",
    "#                 \"message\": f\"Source data for {table_name} does not have publication_number column\"\n",
    "#             }\n",
    "        \n",
    "#         # Get target table\n",
    "#         try:\n",
    "#             target_table = DeltaTable.forName(spark, full_table_name)\n",
    "#         except Exception as e:\n",
    "#             return {\n",
    "#                 \"status\": \"error\",\n",
    "#                 \"message\": f\"Could not access target table {full_table_name}: {str(e)}\"\n",
    "#             }\n",
    "        \n",
    "#         # Perform merge operation\n",
    "#         try:\n",
    "#             target_table.alias(\"target\").merge(\n",
    "#                 new_data_df.alias(\"source\"),\n",
    "#                 \"target.publication_number = source.publication_number\"\n",
    "#             ).whenMatchedUpdateAll(\n",
    "#             ).whenNotMatchedInsertAll(\n",
    "#             ).execute()\n",
    "            \n",
    "#             return {\n",
    "#                 \"status\": \"success\",\n",
    "#                 \"message\": f\"Upsert completed for {table_name}\"\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             return {\n",
    "#                 \"status\": \"error\",\n",
    "#                 \"message\": f\"Error during merge operation for {table_name}: {str(e)}\"\n",
    "#             }\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         return {\n",
    "#             \"status\": \"error\",\n",
    "#             \"message\": f\"Error processing upsert for {table_name}: {str(e)}\"\n",
    "#         }\n",
    "\n",
    "# def register_all_patent_tables(gold_base_path=\"/Volumes/nokia-assginment-catalog/gold\", \n",
    "#                               database_name=\"patent_data\",\n",
    "#                               drop_existing=False):\n",
    "#     \"\"\"\n",
    "#     Register all patent Delta tables in the Databricks metastore\n",
    "    \n",
    "#     Args:\n",
    "#         gold_base_path: Base path to gold Delta tables\n",
    "#         database_name: Database to create tables in\n",
    "#         drop_existing: Whether to drop existing tables\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Results with status for each table\n",
    "#     \"\"\"\n",
    "#     spark = initialize_spark()\n",
    "#     results = {}\n",
    "    \n",
    "#     # Define tables to register\n",
    "#     tables = [\n",
    "#         {\"path\": f\"{gold_base_path}/patents\", \"name\": \"patents\"},\n",
    "#         {\"path\": f\"{gold_base_path}/ipc_classifications\", \"name\": \"ipc_classifications\"},\n",
    "#         {\"path\": f\"{gold_base_path}/ipc_individual\", \"name\": \"ipc_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/inventors\", \"name\": \"inventors\"},\n",
    "#         {\"path\": f\"{gold_base_path}/inventors_individual\", \"name\": \"inventors_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/applicants\", \"name\": \"applicants\"},\n",
    "#         {\"path\": f\"{gold_base_path}/applicants_individual\", \"name\": \"applicants_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/us_applicants\", \"name\": \"us_applicants\"},\n",
    "#         {\"path\": f\"{gold_base_path}/us_applicants_individual\", \"name\": \"us_applicants_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/claims\", \"name\": \"claims\"},\n",
    "#         {\"path\": f\"{gold_base_path}/claims_individual\", \"name\": \"claims_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/complete_patents\", \"name\": \"complete_patents\"}\n",
    "#     ]\n",
    "    \n",
    "#     # Register each table\n",
    "#     for table_info in tables:\n",
    "#         delta_path = table_info[\"path\"]\n",
    "#         table_name = table_info[\"name\"]\n",
    "        \n",
    "#         # Check if Delta path exists\n",
    "#         if not check_path_exists(delta_path):\n",
    "#             print(f\"Delta path does not exist: {delta_path}\")\n",
    "#             results[table_name] = \"Path not found\"\n",
    "#             continue\n",
    "            \n",
    "#         # Check if it's a valid Delta table\n",
    "#         if not is_delta_table(spark, delta_path):\n",
    "#             print(f\"Path is not a valid Delta table: {delta_path}\")\n",
    "#             results[table_name] = \"Not a valid Delta table\"\n",
    "#             continue\n",
    "            \n",
    "#         # Register the table\n",
    "#         success = register_delta_table(\n",
    "#             spark, \n",
    "#             delta_path, \n",
    "#             table_name, \n",
    "#             database_name=database_name, \n",
    "#             drop_if_exists=drop_existing\n",
    "#         )\n",
    "        \n",
    "#         if success:\n",
    "#             results[table_name] = \"Registered successfully\"\n",
    "#         else:\n",
    "#             results[table_name] = \"Registration failed\"\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def create_upsert_function(spark, database_name=\"patent_data\"):\n",
    "#     \"\"\"\n",
    "#     Create a SQL function for upserting data into tables\n",
    "    \n",
    "#     Args:\n",
    "#         spark: SparkSession\n",
    "#         database_name: Database containing the tables\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # For compatibility with your Databricks environment, we'll use a simple table function\n",
    "#         # instead of a stored procedure\n",
    "#         udf_code = f\"\"\"\n",
    "#         CREATE OR REPLACE FUNCTION {database_name}.patent_upsert(table_name STRING, source_path STRING)\n",
    "#         RETURNS STRING\n",
    "#         LANGUAGE PYTHON\n",
    "#         AS\n",
    "#         $$\n",
    "#         from delta.tables import DeltaTable\n",
    "#         import json\n",
    "        \n",
    "#         try:\n",
    "#             # Fully qualified table name\n",
    "#             full_table_name = f\"{database_name}.{{table_name}}\"\n",
    "            \n",
    "#             # Read the new data\n",
    "#             new_data_df = spark.read.format(\"delta\").load(source_path)\n",
    "            \n",
    "#             # Check if publication_number exists in the source data\n",
    "#             if \"publication_number\" not in new_data_df.columns:\n",
    "#                 return json.dumps({{\n",
    "#                     \"status\": \"error\", \n",
    "#                     \"message\": f\"Source data does not have publication_number column\"\n",
    "#                 }})\n",
    "            \n",
    "#             # Get target table\n",
    "#             try:\n",
    "#                 target_table = DeltaTable.forName(spark, full_table_name)\n",
    "#             except Exception as e:\n",
    "#                 return json.dumps({{\n",
    "#                     \"status\": \"error\",\n",
    "#                     \"message\": f\"Could not access target table {{full_table_name}}: {{str(e)}}\"\n",
    "#                 }})\n",
    "            \n",
    "#             # Perform merge operation\n",
    "#             target_table.alias(\"target\").merge(\n",
    "#                 new_data_df.alias(\"source\"),\n",
    "#                 \"target.publication_number = source.publication_number\"\n",
    "#             ).whenMatchedUpdateAll(\n",
    "#             ).whenNotMatchedInsertAll(\n",
    "#             ).execute()\n",
    "            \n",
    "#             return json.dumps({{\n",
    "#                 \"status\": \"success\",\n",
    "#                 \"message\": f\"Upsert completed for {{table_name}}\"\n",
    "#             }})\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             return json.dumps({{\n",
    "#                 \"status\": \"error\",\n",
    "#                 \"message\": f\"Error: {{str(e)}}\"\n",
    "#             }})\n",
    "#         $$\n",
    "#         \"\"\"\n",
    "        \n",
    "#         spark.sql(udf_code)\n",
    "#         print(f\"Created upsert function {database_name}.patent_upsert\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating upsert function: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return False\n",
    "\n",
    "# def batch_upsert_function(spark, database_name=\"patent_data\"):\n",
    "#     \"\"\"\n",
    "#     Create a function to perform batch upserts across all tables\n",
    "    \n",
    "#     Args:\n",
    "#         spark: SparkSession\n",
    "#         database_name: Database containing the tables\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         batch_function_code = f\"\"\"\n",
    "#         CREATE OR REPLACE FUNCTION {database_name}.patent_upsert_job(base_path STRING)\n",
    "#         RETURNS STRING\n",
    "#         LANGUAGE PYTHON\n",
    "#         AS\n",
    "#         $$\n",
    "#         import json\n",
    "#         from delta.tables import DeltaTable\n",
    "        \n",
    "#         results = {{}}\n",
    "        \n",
    "#         # Tables to process\n",
    "#         tables = [\n",
    "#             \"patents\",\n",
    "#             \"ipc_classifications\",\n",
    "#             \"ipc_individual\",\n",
    "#             \"inventors\",\n",
    "#             \"inventors_individual\",\n",
    "#             \"applicants\",\n",
    "#             \"applicants_individual\",\n",
    "#             \"us_applicants\",\n",
    "#             \"us_applicants_individual\",\n",
    "#             \"claims\",\n",
    "#             \"claims_individual\",\n",
    "#             \"complete_patents\"\n",
    "#         ]\n",
    "        \n",
    "#         # Process each table\n",
    "#         for table_name in tables:\n",
    "#             try:\n",
    "#                 # Paths\n",
    "#                 source_path = f\"{{base_path}}/{{table_name}}\"\n",
    "#                 full_table_name = f\"{database_name}.{{table_name}}\"\n",
    "                \n",
    "#                 # Check if path exists\n",
    "#                 try:\n",
    "#                     dbutils.fs.ls(source_path)\n",
    "#                 except:\n",
    "#                     results[table_name] = {{\"status\": \"skipped\", \"message\": \"Source path not found\"}}\n",
    "#                     continue\n",
    "                \n",
    "#                 # Check if it's a valid Delta table\n",
    "#                 try:\n",
    "#                     source_df = spark.read.format(\"delta\").load(source_path)\n",
    "#                 except:\n",
    "#                     results[table_name] = {{\"status\": \"error\", \"message\": \"Not a valid Delta source\"}}\n",
    "#                     continue\n",
    "                \n",
    "#                 # Check if target table exists\n",
    "#                 try:\n",
    "#                     target_table = DeltaTable.forName(spark, full_table_name)\n",
    "#                 except:\n",
    "#                     results[table_name] = {{\"status\": \"error\", \"message\": \"Target table not found\"}}\n",
    "#                     continue\n",
    "                \n",
    "#                 # Check for primary key\n",
    "#                 if \"publication_number\" not in source_df.columns:\n",
    "#                     results[table_name] = {{\"status\": \"error\", \"message\": \"Source missing publication_number\"}}\n",
    "#                     continue\n",
    "                \n",
    "#                 # Perform merge operation\n",
    "#                 target_table.alias(\"target\").merge(\n",
    "#                     source_df.alias(\"source\"),\n",
    "#                     \"target.publication_number = source.publication_number\"\n",
    "#                 ).whenMatchedUpdateAll(\n",
    "#                 ).whenNotMatchedInsertAll(\n",
    "#                 ).execute()\n",
    "                \n",
    "#                 results[table_name] = {{\"status\": \"success\", \"message\": \"Upsert completed\"}}\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 results[table_name] = {{\"status\": \"error\", \"message\": str(e)}}\n",
    "        \n",
    "#         return json.dumps(results)\n",
    "#         $$\n",
    "#         \"\"\"\n",
    "        \n",
    "#         spark.sql(batch_function_code)\n",
    "#         print(f\"Created batch upsert function {database_name}.patent_upsert_job\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating batch upsert function: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return False\n",
    "\n",
    "# def create_control_table(spark, database_name=\"patent_data\"):\n",
    "#     \"\"\"Create a control table for tracking updates\"\"\"\n",
    "#     try:\n",
    "#         spark.sql(f\"\"\"\n",
    "#         CREATE TABLE IF NOT EXISTS {database_name}.patent_update_control (\n",
    "#             update_id STRING,\n",
    "#             update_timestamp TIMESTAMP,\n",
    "#             source_path STRING,\n",
    "#             result STRING,\n",
    "#             PRIMARY KEY (update_id)\n",
    "#         ) USING DELTA\n",
    "#         \"\"\")\n",
    "#         print(f\"Created control table {database_name}.patent_update_control\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating control table: {str(e)}\")\n",
    "#         print(traceback.format_exc())\n",
    "#         return False\n",
    "\n",
    "# def setup_upsert_usage_instructions():\n",
    "#     \"\"\"\n",
    "#     Generate instructions for using the upsert capabilities\n",
    "#     \"\"\"\n",
    "#     instructions = \"\"\"\n",
    "#     -- To upsert data for a single table:\n",
    "#     SELECT patent_data.patent_upsert('patents', '/path/to/new/patents/data');\n",
    "    \n",
    "#     -- To batch upsert data for all tables from a base directory:\n",
    "#     SELECT patent_data.patent_upsert_job('/path/to/new/data');\n",
    "    \n",
    "#     -- To track updates in the control table:\n",
    "#     INSERT INTO patent_data.patent_update_control\n",
    "#     VALUES (\n",
    "#         uuid(), -- Generate a unique ID\n",
    "#         current_timestamp(),\n",
    "#         '/path/to/processed/data',\n",
    "#         patent_data.patent_upsert_job('/path/to/processed/data')\n",
    "#     );\n",
    "    \n",
    "#     -- To set up automatic monitoring with a Databricks job:\n",
    "#     -- 1. Create a notebook with the update logic\n",
    "#     -- 2. Schedule it to run on your preferred interval\n",
    "#     -- 3. Use the control table to track processed data and avoid duplicates\n",
    "#     \"\"\"\n",
    "#     return instructions\n",
    "\n",
    "# def incremental_load_instructions():\n",
    "#     \"\"\"\n",
    "#     Generate instructions for incremental loading\n",
    "#     \"\"\"\n",
    "#     instructions = \"\"\"\n",
    "#     -- Example notebook code for incremental loading:\n",
    "    \n",
    "#     # Check the last processed timestamp\n",
    "#     last_process_df = spark.sql('''\n",
    "#         SELECT MAX(update_timestamp) AS last_update \n",
    "#         FROM patent_data.patent_update_control\n",
    "#     ''')\n",
    "    \n",
    "#     last_timestamp = last_process_df.first().last_update\n",
    "    \n",
    "#     if last_timestamp is None:\n",
    "#         last_timestamp = '2000-01-01 00:00:00'\n",
    "    \n",
    "#     # Look for new files to process\n",
    "#     new_files = dbutils.fs.ls('/path/to/incoming/data')\n",
    "    \n",
    "#     # Filter for files newer than last processed\n",
    "#     # Note: This depends on your specific file structure\n",
    "#     # You'll need to adapt this to your actual data organization\n",
    "    \n",
    "#     # If new files exist, process them\n",
    "#     if len(new_files) > 0:\n",
    "#         # Process new files and get path of processed data\n",
    "#         processed_path = '/path/to/processed/output'\n",
    "        \n",
    "#         # Update all tables with the new data\n",
    "#         result = spark.sql(f\"SELECT patent_data.patent_upsert_job('{processed_path}')\")\n",
    "#         result_json = result.first()[0]\n",
    "        \n",
    "#         # Record the update in the control table\n",
    "#         spark.sql(f'''\n",
    "#             INSERT INTO patent_data.patent_update_control\n",
    "#             VALUES (\n",
    "#                 uuid(),\n",
    "#                 current_timestamp(),\n",
    "#                 '{processed_path}',\n",
    "#                 '{result_json}'\n",
    "#             )\n",
    "#         ''')\n",
    "#     \"\"\"\n",
    "#     return instructions\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main function to setup all tables and upsert mechanisms\"\"\"\n",
    "    \n",
    "#     spark = initialize_spark()\n",
    "    \n",
    "#     # Create the database\n",
    "#     database_name = \"patent_data\"\n",
    "#     spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    \n",
    "#     # Create the control table for monitoring updates\n",
    "#     create_control_table(spark, database_name)\n",
    "    \n",
    "#     # Base path to gold Delta tables\n",
    "#     gold_base_path = \"/Volumes/nokia-assginment-catalog/gold\"\n",
    "    \n",
    "#     # Register all tables\n",
    "#     print(\"Registering patent Delta tables...\")\n",
    "#     results = register_all_patent_tables(\n",
    "#         gold_base_path=gold_base_path,\n",
    "#         database_name=database_name,\n",
    "#         drop_existing=False  # Set to True if you want to drop and recreate tables\n",
    "#     )\n",
    "    \n",
    "#     # Print registration results\n",
    "#     for table, status in results.items():\n",
    "#         print(f\"Table {table}: {status}\")\n",
    "    \n",
    "#     # Create upsert functions instead of procedures\n",
    "#     create_upsert_function(spark, database_name)\n",
    "#     batch_upsert_function(spark, database_name)\n",
    "    \n",
    "#     # Print usage instructions\n",
    "#     print(\"\\nTo upsert data into tables, use the following functions:\")\n",
    "#     print(setup_upsert_usage_instructions())\n",
    "    \n",
    "#     print(\"\\nTo set up incremental loading, here's a template:\")\n",
    "#     print(incremental_load_instructions())\n",
    "    \n",
    "#     print(\"\\nSetup complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def9881b-befa-48c5-86a4-11eb208eb9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76da6c7-5fd2-458a-8935-05fadba5827c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from delta.tables import DeltaTable\n",
    "# import traceback\n",
    "# import json\n",
    "# import uuid\n",
    "\n",
    "# def initialize_spark():\n",
    "#     \"\"\"Initialize Spark session with Delta Lake support\"\"\"\n",
    "#     return SparkSession.builder \\\n",
    "#         .appName(\"Patent Delta Tables Registration\") \\\n",
    "#         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "# def check_path_exists(path):\n",
    "#     \"\"\"Check if a path exists and is accessible\"\"\"\n",
    "#     try:\n",
    "#         dbutils.fs.ls(path)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# def is_delta_table(spark, path):\n",
    "#     \"\"\"Check if a path is a valid Delta table\"\"\"\n",
    "#     try:\n",
    "#         DeltaTable.forPath(spark, path)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# def register_and_upsert_table(spark, delta_path, table_name, database_name=\"patent_data\"):\n",
    "#     \"\"\"\n",
    "#     Register a Delta table if it doesn't exist and upsert data\n",
    "    \n",
    "#     Args:\n",
    "#         spark: SparkSession\n",
    "#         delta_path: Path to Delta table\n",
    "#         table_name: Name of the table to create/update\n",
    "#         database_name: Database to create the table in\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Operation result\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Create database if it doesn't exist\n",
    "#         spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        \n",
    "#         # Fully qualified table name\n",
    "#         full_table_name = f\"{database_name}.{table_name}\"\n",
    "        \n",
    "#         # Read the Delta data (source)\n",
    "#         source_df = spark.read.format(\"delta\").load(delta_path)\n",
    "        \n",
    "#         # Check if source has data\n",
    "#         source_count = source_df.count()\n",
    "#         if source_count == 0:\n",
    "#             return {\n",
    "#                 \"status\": \"warning\",\n",
    "#                 \"message\": f\"Source Delta file at {delta_path} is empty\"\n",
    "#             }\n",
    "            \n",
    "#         # Check if table already exists\n",
    "#         table_exists = False\n",
    "#         existing_tables = [t.name for t in spark.catalog.listTables(database_name)]\n",
    "#         if table_name in existing_tables:\n",
    "#             table_exists = True\n",
    "            \n",
    "#         # For individual tables, use overwrite mode instead of merge\n",
    "#         is_individual_table = \"_individual\" in table_name\n",
    "        \n",
    "#         if not table_exists:\n",
    "#             # Create table if it doesn't exist\n",
    "#             print(f\"Creating table {full_table_name} from {delta_path}\")\n",
    "#             source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            \n",
    "#             # Configure table properties for optimization\n",
    "#             spark.sql(f\"\"\"\n",
    "#                 ALTER TABLE {full_table_name}\n",
    "#                 SET TBLPROPERTIES (\n",
    "#                     'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "#                     'delta.autoOptimize.autoCompact' = 'true',\n",
    "#                     'delta.enableChangeDataFeed' = 'true',\n",
    "#                     'delta.logRetentionDuration' = '30 days',\n",
    "#                     'delta.tuneFileSizesForRewrites' = 'true'\n",
    "#                 )\n",
    "#             \"\"\")\n",
    "            \n",
    "#             print(f\"Table {full_table_name} created successfully with {source_count} rows\")\n",
    "#             return {\n",
    "#                 \"status\": \"created\",\n",
    "#                 \"message\": f\"Table {full_table_name} created with {source_count} rows\"\n",
    "#             }\n",
    "#         elif is_individual_table:\n",
    "#             # For individual tables, use complete replacement instead of merge\n",
    "#             print(f\"Replacing data in {full_table_name} from {delta_path} (individual table)\")\n",
    "#             source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "#             print(f\"Data in {full_table_name} replaced successfully with {source_count} rows\")\n",
    "#             return {\n",
    "#                 \"status\": \"replaced\",\n",
    "#                 \"message\": f\"Table {full_table_name} completely replaced with {source_count} rows\"\n",
    "#             }\n",
    "#         else:\n",
    "#             # For regular tables, perform upsert\n",
    "#             if \"publication_number\" not in source_df.columns:\n",
    "#                 return {\n",
    "#                     \"status\": \"warning\",\n",
    "#                     \"message\": f\"Source data doesn't have publication_number column\"\n",
    "#                 }\n",
    "                \n",
    "#             print(f\"Performing upsert to {full_table_name} from {delta_path}\")\n",
    "            \n",
    "#             # Get target table\n",
    "#             target_table = DeltaTable.forName(spark, full_table_name)\n",
    "            \n",
    "#             # Perform merge operation\n",
    "#             target_table.alias(\"target\").merge(\n",
    "#                 source_df.alias(\"source\"),\n",
    "#                 \"target.publication_number = source.publication_number\"\n",
    "#             ).whenMatchedUpdateAll(\n",
    "#             ).whenNotMatchedInsertAll(\n",
    "#             ).execute()\n",
    "            \n",
    "#             print(f\"Upsert to {full_table_name} completed successfully\")\n",
    "#             return {\n",
    "#                 \"status\": \"updated\",\n",
    "#                 \"message\": f\"Table {full_table_name} updated with upsert\"\n",
    "#             }\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         error_message = f\"Error processing {table_name}: {str(e)}\"\n",
    "#         print(error_message)\n",
    "#         print(traceback.format_exc())\n",
    "#         return {\n",
    "#             \"status\": \"error\",\n",
    "#             \"message\": error_message\n",
    "#         }\n",
    "\n",
    "# def register_and_upsert_all_tables(gold_base_path=\"/Volumes/nokia-assginment-catalog/gold\", \n",
    "#                                    database_name=\"patent_data\"):\n",
    "#     \"\"\"\n",
    "#     Process all patent gold tables - register if needed and upsert data\n",
    "    \n",
    "#     Args:\n",
    "#         gold_base_path: Base path to gold Delta tables\n",
    "#         database_name: Database to create tables in\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Results for each table\n",
    "#     \"\"\"\n",
    "#     spark = initialize_spark()\n",
    "#     results = {}\n",
    "    \n",
    "#     # Create control table if it doesn't exist\n",
    "#     try:\n",
    "#         spark.sql(f\"\"\"\n",
    "#         CREATE TABLE IF NOT EXISTS {database_name}.patent_update_control (\n",
    "#             update_id STRING,\n",
    "#             update_timestamp TIMESTAMP,\n",
    "#             result STRING\n",
    "#         ) USING DELTA\n",
    "#         \"\"\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Warning: Could not create control table: {str(e)}\")\n",
    "    \n",
    "#     # Define tables to process\n",
    "#     tables = [\n",
    "#         {\"path\": f\"{gold_base_path}/patents\", \"name\": \"patents\"},\n",
    "#         {\"path\": f\"{gold_base_path}/ipc_classifications\", \"name\": \"ipc_classifications\"},\n",
    "#         {\"path\": f\"{gold_base_path}/ipc_individual\", \"name\": \"ipc_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/inventors\", \"name\": \"inventors\"},\n",
    "#         {\"path\": f\"{gold_base_path}/inventors_individual\", \"name\": \"inventors_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/applicants\", \"name\": \"applicants\"},\n",
    "#         {\"path\": f\"{gold_base_path}/applicants_individual\", \"name\": \"applicants_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/us_applicants\", \"name\": \"us_applicants\"},\n",
    "#         {\"path\": f\"{gold_base_path}/us_applicants_individual\", \"name\": \"us_applicants_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/claims\", \"name\": \"claims\"},\n",
    "#         {\"path\": f\"{gold_base_path}/claims_individual\", \"name\": \"claims_individual\"},\n",
    "#         {\"path\": f\"{gold_base_path}/complete_patents\", \"name\": \"complete_patents\"}\n",
    "#     ]\n",
    "    \n",
    "#     # Register and upsert each table\n",
    "#     for table_info in tables:\n",
    "#         delta_path = table_info[\"path\"]\n",
    "#         table_name = table_info[\"name\"]\n",
    "        \n",
    "#         # Check if Delta path exists\n",
    "#         if not check_path_exists(delta_path):\n",
    "#             print(f\"Delta path does not exist: {delta_path}\")\n",
    "#             results[table_name] = {\"status\": \"skipped\", \"message\": \"Delta path not found\"}\n",
    "#             continue\n",
    "            \n",
    "#         # Check if it's a valid Delta table\n",
    "#         if not is_delta_table(spark, delta_path):\n",
    "#             print(f\"Path is not a valid Delta table: {delta_path}\")\n",
    "#             results[table_name] = {\"status\": \"skipped\", \"message\": \"Not a valid Delta table\"}\n",
    "#             continue\n",
    "            \n",
    "#         # Register and upsert the table\n",
    "#         result = register_and_upsert_table(\n",
    "#             spark, \n",
    "#             delta_path, \n",
    "#             table_name, \n",
    "#             database_name=database_name\n",
    "#         )\n",
    "        \n",
    "#         results[table_name] = result\n",
    "    \n",
    "#     # Log this batch of updates in the control table\n",
    "#     try:\n",
    "#         result_json = json.dumps(results)\n",
    "#         update_id = str(uuid.uuid4())\n",
    "        \n",
    "#         spark.sql(f\"\"\"\n",
    "#         INSERT INTO {database_name}.patent_update_control\n",
    "#         VALUES (\n",
    "#             '{update_id}',\n",
    "#             current_timestamp(),\n",
    "#             '{result_json}'\n",
    "#         )\n",
    "#         \"\"\")\n",
    "        \n",
    "#         print(f\"Update logged in control table with ID: {update_id}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Warning: Could not log update in control table: {str(e)}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main function that runs as part of the workflow after gold process notebook\"\"\"\n",
    "    \n",
    "#     # Base path to gold Delta tables\n",
    "#     gold_base_path = \"/Volumes/nokia-assginment-catalog/gold\"\n",
    "#     database_name = \"patent_data\"\n",
    "    \n",
    "#     print(f\"Starting patent data registration and upsert process\")\n",
    "#     print(f\"Source: {gold_base_path}\")\n",
    "#     print(f\"Target database: {database_name}\")\n",
    "    \n",
    "#     # Register and upsert all tables\n",
    "#     results = register_and_upsert_all_tables(\n",
    "#         gold_base_path=gold_base_path,\n",
    "#         database_name=database_name\n",
    "#     )\n",
    "    \n",
    "#     # Print summary\n",
    "#     created_count = sum(1 for r in results.values() if r.get('status') == 'created')\n",
    "#     updated_count = sum(1 for r in results.values() if r.get('status') == 'updated')\n",
    "#     replaced_count = sum(1 for r in results.values() if r.get('status') == 'replaced')\n",
    "#     error_count = sum(1 for r in results.values() if r.get('status') == 'error')\n",
    "#     skipped_count = sum(1 for r in results.values() if r.get('status') == 'skipped')\n",
    "#     warning_count = sum(1 for r in results.values() if r.get('status') == 'warning')\n",
    "    \n",
    "#     print(\"\\n=== PROCESSING SUMMARY ===\")\n",
    "#     print(f\"Tables created: {created_count}\")\n",
    "#     print(f\"Tables updated via upsert: {updated_count}\")\n",
    "#     print(f\"Tables completely replaced: {replaced_count}\")\n",
    "#     print(f\"Tables with errors: {error_count}\")\n",
    "#     print(f\"Tables skipped: {skipped_count}\")\n",
    "#     print(f\"Tables with warnings: {warning_count}\")\n",
    "    \n",
    "#     # Print details of any errors\n",
    "#     if error_count > 0:\n",
    "#         print(\"\\n=== ERROR DETAILS ===\")\n",
    "#         for table, result in results.items():\n",
    "#             if result.get('status') == 'error':\n",
    "#                 print(f\"{table}: {result.get('message')}\")\n",
    "    \n",
    "#     print(\"\\nProcess completed\")\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b016ff7-4d4a-4066-8cec-2a8e9b07e394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056091dd-09f8-4313-95c9-8c16f2b1a620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eda366a-0657-470e-a711-03008a94bb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "import traceback\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session with Delta Lake support\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Patent Delta Tables Registration\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def check_path_exists(path):\n",
    "    \"\"\"Check if a path exists and is accessible\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_delta_table(spark, path):\n",
    "    \"\"\"Check if a path is a valid Delta table\"\"\"\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_new_data_processed():\n",
    "    \"\"\"\n",
    "    Check if new data was processed in the previous gold layer step\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether new data was processed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Method 1: Check processing metadata table\n",
    "        spark = initialize_spark()\n",
    "        \n",
    "        # Check if the processing_metadata table exists\n",
    "        database_name = \"patent_data\"\n",
    "        try:\n",
    "            metadata_df = spark.sql(f\"SELECT * FROM {database_name}.processing_metadata ORDER BY processing_timestamp DESC LIMIT 1\")\n",
    "            if metadata_df.count() > 0:\n",
    "                last_record = metadata_df.first()\n",
    "                new_data_processed = last_record.new_data_processed\n",
    "                print(f\"Found processing metadata: new_data_processed={new_data_processed}\")\n",
    "                return new_data_processed\n",
    "        except:\n",
    "            print(\"Could not query processing_metadata table\")\n",
    "        \n",
    "        # Method 2: Check processing status Delta file\n",
    "        try:\n",
    "            processing_status_path = \"/Volumes/nokia-assginment-catalog/processing_status/status\"\n",
    "            if check_path_exists(processing_status_path):\n",
    "                status_df = spark.read.format(\"delta\").load(processing_status_path)\n",
    "                if status_df.count() > 0:\n",
    "                    status = status_df.first().new_data_processed\n",
    "                    print(f\"Found processing status file: new_data_processed={status}\")\n",
    "                    return status\n",
    "        except:\n",
    "            print(\"Could not read processing status file\")\n",
    "        \n",
    "        # Method 3: Check for previous notebook result\n",
    "        try:\n",
    "            # Try to get result from previous notebook\n",
    "            prev_result_str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().parentContext().get(\"result\")\n",
    "            if prev_result_str:\n",
    "                prev_result = json.loads(prev_result_str)\n",
    "                if \"new_data_processed\" in prev_result:\n",
    "                    status = prev_result[\"new_data_processed\"]\n",
    "                    print(f\"Found previous notebook result: new_data_processed={status}\")\n",
    "                    return status\n",
    "        except:\n",
    "            print(\"Could not get previous notebook result\")\n",
    "        \n",
    "        # Default: If we can't determine, assume there might be new data\n",
    "        print(\"Could not determine if new data was processed, assuming yes\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking for new data: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        # If there's an error, assume there might be new data\n",
    "        return True\n",
    "\n",
    "def register_and_upsert_table(spark, delta_path, table_name, database_name=\"patent_data\"):\n",
    "    \"\"\"\n",
    "    Register a Delta table if it doesn't exist and upsert data\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        delta_path: Path to Delta table\n",
    "        table_name: Name of the table to create/update\n",
    "        database_name: Database to create the table in\n",
    "    \n",
    "    Returns:\n",
    "        dict: Operation result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Debug - what's in the gold delta path?\n",
    "        print(f\"Contents of {delta_path}:\")\n",
    "        path_files = dbutils.fs.ls(delta_path)\n",
    "        for file in path_files[:5]:  # Show first 5 items\n",
    "            print(f\"  - {file.name} ({file.size} bytes)\")\n",
    "        \n",
    "        # Debug - how many batches are in silver layer\n",
    "        try:\n",
    "            silver_path = \"/Volumes/nokia-assginment-catalog/silver/patent_data\"\n",
    "            silver_dirs = [d for d in dbutils.fs.ls(silver_path) if d.isDir() and not d.name.startswith('_')]\n",
    "            print(f\"Silver layer has {len(silver_dirs)} batch directories\")\n",
    "        except:\n",
    "            print(\"Could not access silver layer to count batches\")\n",
    "        \n",
    "        # Debug - how many batches have checkpoints\n",
    "        try:\n",
    "            checkpoint_location = \"/Volumes/nokia-assginment-catalog/checkpoints/checkpoints_data/gold_autoloader/\"\n",
    "            if check_path_exists(checkpoint_location):\n",
    "                checkpoints = [d for d in dbutils.fs.ls(checkpoint_location) if not d.name.startswith('_')]\n",
    "                print(f\"Found {len(checkpoints)} checkpoint files\")\n",
    "        except:\n",
    "            print(\"Could not check checkpoint files\")\n",
    "\n",
    "        # Create database if it doesn't exist\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        \n",
    "        # Fully qualified table name\n",
    "        full_table_name = f\"{database_name}.{table_name}\"\n",
    "        \n",
    "        # Read the Delta data (source)\n",
    "        source_df = spark.read.format(\"delta\").load(delta_path)\n",
    "        \n",
    "        # Check if source has data and print more details for debugging\n",
    "        source_count = source_df.count()\n",
    "        print(f\"Source Delta file at {delta_path} has {source_count} records\")\n",
    "        \n",
    "        if source_count == 0:\n",
    "            return {\n",
    "                \"status\": \"warning\",\n",
    "                \"message\": f\"Source Delta file at {delta_path} is empty\"\n",
    "            }\n",
    "            \n",
    "        # Check if table already exists\n",
    "        table_exists = False\n",
    "        existing_tables = [t.name for t in spark.catalog.listTables(database_name)]\n",
    "        if table_name in existing_tables:\n",
    "            table_exists = True\n",
    "            \n",
    "        # For individual tables, use overwrite mode instead of merge\n",
    "        is_individual_table = \"_individual\" in table_name\n",
    "        \n",
    "        if not table_exists:\n",
    "            # Create table if it doesn't exist\n",
    "            print(f\"Creating table {full_table_name} from {delta_path}\")\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            \n",
    "            # Configure table properties for optimization\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {full_table_name}\n",
    "                SET TBLPROPERTIES (\n",
    "                    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "                    'delta.autoOptimize.autoCompact' = 'true',\n",
    "                    'delta.enableChangeDataFeed' = 'true',\n",
    "                    'delta.logRetentionDuration' = '30 days',\n",
    "                    'delta.tuneFileSizesForRewrites' = 'true'\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"Table {full_table_name} created successfully with {source_count} rows\")\n",
    "            return {\n",
    "                \"status\": \"created\",\n",
    "                \"message\": f\"Table {full_table_name} created with {source_count} rows\"\n",
    "            }\n",
    "        elif is_individual_table:\n",
    "            # For individual tables, keep a record of how many rows we're about to replace\n",
    "            target_df = spark.read.table(full_table_name)\n",
    "            target_count = target_df.count()\n",
    "            print(f\"Replacing {target_count} existing rows with {source_count} rows in {full_table_name}\")\n",
    "            \n",
    "            # If source has fewer records than target, this could indicate a problem\n",
    "            if source_count < target_count and target_count > 0:\n",
    "                print(f\"WARNING: Source has fewer records ({source_count}) than existing table ({target_count})\")\n",
    "            \n",
    "            # For individual tables, use complete replacement instead of merge\n",
    "            source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            print(f\"Data in {full_table_name} replaced successfully with {source_count} rows\")\n",
    "            return {\n",
    "                \"status\": \"replaced\",\n",
    "                \"message\": f\"Table {full_table_name} completely replaced with {source_count} rows (previous: {target_count})\"\n",
    "            }\n",
    "        else:\n",
    "            # For regular tables, perform upsert\n",
    "            if \"publication_number\" not in source_df.columns:\n",
    "                return {\n",
    "                    \"status\": \"warning\",\n",
    "                    \"message\": f\"Source data doesn't have publication_number column\"\n",
    "                }\n",
    "            \n",
    "            # Get target table and count before merge\n",
    "            target_table = DeltaTable.forName(spark, full_table_name)\n",
    "            target_count_before = spark.read.table(full_table_name).count()\n",
    "            \n",
    "            print(f\"Performing upsert to {full_table_name} from {delta_path}\")\n",
    "            print(f\"Target table has {target_count_before} rows before merge\")\n",
    "            \n",
    "            # Sample a few publication numbers from source for debugging\n",
    "            sample_pubs = source_df.select(\"publication_number\").limit(5).collect()\n",
    "            print(f\"Sample publication numbers from source: {[row.publication_number for row in sample_pubs]}\")\n",
    "            \n",
    "            # Perform merge operation\n",
    "            target_table.alias(\"target\").merge(\n",
    "                source_df.alias(\"source\"),\n",
    "                \"target.publication_number = source.publication_number\"\n",
    "            ).whenMatchedUpdateAll(\n",
    "            ).whenNotMatchedInsertAll(\n",
    "            ).execute()\n",
    "            \n",
    "            # Count after merge to see if anything changed\n",
    "            target_count_after = spark.read.table(full_table_name).count()\n",
    "            rows_changed = target_count_after - target_count_before\n",
    "            \n",
    "            print(f\"Upsert to {full_table_name} completed. Rows before: {target_count_before}, after: {target_count_after}\")\n",
    "            print(f\"Net change in rows: {rows_changed}\")\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"updated\",\n",
    "                \"message\": f\"Table {full_table_name} updated with upsert. Net new rows: {rows_changed}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing {table_name}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": error_message\n",
    "        }\n",
    "\n",
    "def register_and_upsert_all_tables(gold_base_path=\"/Volumes/nokia-assginment-catalog/gold\", \n",
    "                                   database_name=\"patent_data\"):\n",
    "    \"\"\"\n",
    "    Process all patent gold tables - register if needed and upsert data\n",
    "    \n",
    "    Args:\n",
    "        gold_base_path: Base path to gold Delta tables\n",
    "        database_name: Database to create tables in\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results for each table\n",
    "    \"\"\"\n",
    "    spark = initialize_spark()\n",
    "    results = {}\n",
    "    \n",
    "    # Create control table if it doesn't exist\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {database_name}.patent_update_control (\n",
    "            update_id STRING,\n",
    "            update_timestamp TIMESTAMP,\n",
    "            result STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create control table: {str(e)}\")\n",
    "    \n",
    "    # Define tables to process\n",
    "    tables = [\n",
    "        {\"path\": f\"{gold_base_path}/patents\", \"name\": \"patents\"},\n",
    "        {\"path\": f\"{gold_base_path}/ipc_classifications\", \"name\": \"ipc_classifications\"},\n",
    "        {\"path\": f\"{gold_base_path}/ipc_individual\", \"name\": \"ipc_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/inventors\", \"name\": \"inventors\"},\n",
    "        {\"path\": f\"{gold_base_path}/inventors_individual\", \"name\": \"inventors_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/applicants\", \"name\": \"applicants\"},\n",
    "        {\"path\": f\"{gold_base_path}/applicants_individual\", \"name\": \"applicants_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/us_applicants\", \"name\": \"us_applicants\"},\n",
    "        {\"path\": f\"{gold_base_path}/us_applicants_individual\", \"name\": \"us_applicants_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/claims\", \"name\": \"claims\"},\n",
    "        {\"path\": f\"{gold_base_path}/claims_individual\", \"name\": \"claims_individual\"},\n",
    "        {\"path\": f\"{gold_base_path}/complete_patents\", \"name\": \"complete_patents\"}\n",
    "    ]\n",
    "    \n",
    "    # Register and upsert each table\n",
    "    for table_info in tables:\n",
    "        delta_path = table_info[\"path\"]\n",
    "        table_name = table_info[\"name\"]\n",
    "        \n",
    "        # Check if Delta path exists\n",
    "        if not check_path_exists(delta_path):\n",
    "            print(f\"Delta path does not exist: {delta_path}\")\n",
    "            results[table_name] = {\"status\": \"skipped\", \"message\": \"Delta path not found\"}\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a valid Delta table\n",
    "        if not is_delta_table(spark, delta_path):\n",
    "            print(f\"Path is not a valid Delta table: {delta_path}\")\n",
    "            results[table_name] = {\"status\": \"skipped\", \"message\": \"Not a valid Delta table\"}\n",
    "            continue\n",
    "            \n",
    "        # Register and upsert the table\n",
    "        result = register_and_upsert_table(\n",
    "            spark, \n",
    "            delta_path, \n",
    "            table_name, \n",
    "            database_name=database_name\n",
    "        )\n",
    "        \n",
    "        results[table_name] = result\n",
    "    \n",
    "    # Log this batch of updates in the control table\n",
    "    try:\n",
    "        result_json = json.dumps(results)\n",
    "        update_id = str(uuid.uuid4())\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO {database_name}.patent_update_control\n",
    "        VALUES (\n",
    "            '{update_id}',\n",
    "            current_timestamp(),\n",
    "            '{result_json}'\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"Update logged in control table with ID: {update_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not log update in control table: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function that runs as part of the workflow after gold process notebook\"\"\"\n",
    "    \n",
    "    # Check if we should run based on whether new data was processed in the previous step\n",
    "    try:\n",
    "        dbutils.widgets.dropdown(\"force_update\", \"false\", [\"true\", \"false\"], \"Force Table Update\")\n",
    "        force_update = dbutils.widgets.get(\"force_update\") == \"true\"\n",
    "    except:\n",
    "        force_update = False\n",
    "    \n",
    "    # Check if new data was processed in the previous step\n",
    "    new_data_processed = check_new_data_processed()\n",
    "    \n",
    "    # Skip processing if no new data and not forcing update\n",
    "    if not new_data_processed and not force_update:\n",
    "        print(\"No new data was processed and force_update=false, skipping table registration\")\n",
    "        result = {\n",
    "            \"status\": \"skipped\",\n",
    "            \"message\": \"No new data was processed and force_update=false\"\n",
    "        }\n",
    "        dbutils.notebook.exit(json.dumps(result))\n",
    "        return\n",
    "    \n",
    "    # Base path to gold Delta tables\n",
    "    gold_base_path = \"/Volumes/nokia-assginment-catalog/gold\"\n",
    "    database_name = \"patent_data\"\n",
    "    \n",
    "    print(f\"Starting patent data registration and upsert process\")\n",
    "    if new_data_processed:\n",
    "        print(\"New data was processed in previous step\")\n",
    "    if force_update:\n",
    "        print(\"Force update requested\")\n",
    "        \n",
    "    print(f\"Source: {gold_base_path}\")\n",
    "    print(f\"Target database: {database_name}\")\n",
    "    \n",
    "    # Register and upsert all tables\n",
    "    results = register_and_upsert_all_tables(\n",
    "        gold_base_path=gold_base_path,\n",
    "        database_name=database_name\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    created_count = sum(1 for r in results.values() if r.get('status') == 'created')\n",
    "    updated_count = sum(1 for r in results.values() if r.get('status') == 'updated')\n",
    "    replaced_count = sum(1 for r in results.values() if r.get('status') == 'replaced')\n",
    "    error_count = sum(1 for r in results.values() if r.get('status') == 'error')\n",
    "    skipped_count = sum(1 for r in results.values() if r.get('status') == 'skipped')\n",
    "    warning_count = sum(1 for r in results.values() if r.get('status') == 'warning')\n",
    "    \n",
    "    print(\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Tables created: {created_count}\")\n",
    "    print(f\"Tables updated via upsert: {updated_count}\")\n",
    "    print(f\"Tables completely replaced: {replaced_count}\")\n",
    "    print(f\"Tables with errors: {error_count}\")\n",
    "    print(f\"Tables skipped: {skipped_count}\")\n",
    "    print(f\"Tables with warnings: {warning_count}\")\n",
    "    \n",
    "    # Print details of any errors\n",
    "    if error_count > 0:\n",
    "        print(\"\\n=== ERROR DETAILS ===\")\n",
    "        for table, result in results.items():\n",
    "            if result.get('status') == 'error':\n",
    "                print(f\"{table}: {result.get('message')}\")\n",
    "    \n",
    "    print(\"\\nProcess completed\")\n",
    "    \n",
    "    # Return results as JSON\n",
    "    final_result = {\n",
    "        \"status\": \"completed\",\n",
    "        \"created\": created_count,\n",
    "        \"updated\": updated_count,\n",
    "        \"replaced\": replaced_count,\n",
    "        \"errors\": error_count,\n",
    "        \"skipped\": skipped_count,\n",
    "        \"warnings\": warning_count\n",
    "    }\n",
    "    dbutils.notebook.exit(json.dumps(final_result))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3311438205404710,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "register_delta_tables_in_sql_warehouse",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
