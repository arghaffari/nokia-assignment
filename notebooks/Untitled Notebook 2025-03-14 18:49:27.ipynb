{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c57a3c4-2371-4d2c-b4aa-418b405a7139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lxml\n",
    "%pip install graphviz -U --quiet\n",
    "%pip install networkx -U --quiet\n",
    "%pip install matplotlib -U --quiet\n",
    "%pip install pydot -U --quiet\n",
    "%pip install beautifulsoup4 -U --quiet\n",
    "\n",
    "\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab7cd53-542e-4707-9e2c-a185d2f29810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/Volumes/nokia_assignment_472210556114873/default/nokia_assignment_managed_volume/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a878a26-7d58-4ac2-99e7-65aa40f70397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the HTML tags to remove (keep their content)\n",
    "html_tags = {\n",
    "    \"b\", \"i\", \"u\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \n",
    "    \"span\", \"div\", \"br\", \"strong\", \"em\", \"sub\", \"sup\", \"drawings\", \n",
    "    \"figure\", \"img\", \"ol\", \"ul\", \"li\", \"ol\", \"table\", \"tr\", \"td\", \"o\", \"us-math\", \"us-chemistry\", \"us-sequence-list-doc\", \"sequence-list\",\n",
    "    \"th\", \"tbody\", \"thead\", \"tfoot\", \"figref\", \"description-of-drawings\", \"summary-of-invention\", \"brief-description-of-drawings\"\n",
    "}\n",
    "\n",
    "def clean_html_tags(xml_content):\n",
    "    \"\"\"\n",
    "    Remove specified HTML tags while preserving their content and maintaining XML structure\n",
    "    \"\"\"\n",
    "    # First, preserve the XML declaration if present\n",
    "    if '<?xml' in xml_content:\n",
    "        xml_declaration = xml_content[:xml_content.find('?>') + 2]\n",
    "        main_content = xml_content[xml_content.find('?>') + 2:]\n",
    "    else:\n",
    "        xml_declaration = ''\n",
    "        main_content = xml_content\n",
    "\n",
    "    soup = BeautifulSoup(main_content, 'xml')\n",
    "    \n",
    "    # Find all tags that match our html_tags set\n",
    "    for tag in soup.find_all(html_tags):\n",
    "        # Replace the tag with its contents while preserving attributes\n",
    "        if tag.string:\n",
    "            tag.replace_with(tag.string)\n",
    "        else:\n",
    "            tag.unwrap()\n",
    "    \n",
    "    return xml_declaration + str(soup)\n",
    "\n",
    "def process_xml_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process the XML file to remove HTML tags and save to a new file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Read the entire file\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Find the root tag\n",
    "    root_start = content.find('<us-patent-applications>')\n",
    "    if root_start == -1:\n",
    "        root_start = content.find('<us-patent-application')\n",
    "    \n",
    "    # Split XML declaration and root opening tag\n",
    "    xml_declaration = content[:root_start]\n",
    "    \n",
    "    # Write the start of the file\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(xml_declaration)\n",
    "        if '<us-patent-applications>' in content:\n",
    "            outfile.write('<us-patent-applications>\\n')\n",
    "        \n",
    "        # Process each patent application separately\n",
    "        start_tag = '<us-patent-application'\n",
    "        end_tag = '</us-patent-application>'\n",
    "        \n",
    "        pos = content.find(start_tag)\n",
    "        while pos != -1:\n",
    "            # Find the end of this patent application\n",
    "            end_pos = content.find(end_tag, pos) + len(end_tag)\n",
    "            if end_pos == -1:\n",
    "                break\n",
    "            \n",
    "            # Extract and clean this patent application\n",
    "            patent_xml = content[pos:end_pos]\n",
    "            cleaned_patent = clean_html_tags(patent_xml)\n",
    "            outfile.write(cleaned_patent + '\\n')\n",
    "            \n",
    "            # Move to next patent application\n",
    "            pos = content.find(start_tag, end_pos)\n",
    "        \n",
    "        # Close the root element if it exists\n",
    "        if '<us-patent-applications>' in content:\n",
    "            outfile.write('</us-patent-applications>')\n",
    "\n",
    "def read_with_spark(cleaned_xml_path):\n",
    "    \"\"\"\n",
    "    Read the cleaned XML file using Spark\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"XML Patent Reader\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.legacy.allowUntypedScalaUDF\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Read XML files into DataFrame\n",
    "    df = (\n",
    "        spark.read.format(\"xml\")\n",
    "        .option(\"rowTag\", \"us-patent-application\")\n",
    "        .option(\"charset\", \"UTF-8\")\n",
    "        .option(\"ignoreSurroundingSpaces\", \"true\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .option(\"excludeAttribute\", \"true\")\n",
    "        .option(\"valueTag\", \"_VALUE\")\n",
    "        .load(cleaned_xml_path)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage example\n",
    "input_xml_path = \"/Volumes/nokia_assignment_472210556114873/default/nokia_assignment_managed_volume/batch1.xml\"\n",
    "output_xml_path = \"/Volumes/cleaned/default/cleaned_batches/batch1_cleaned.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee4659ec-837f-4812-a041-5899f8217ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clean the XML file\n",
    "process_xml_file(input_xml_path, output_xml_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad6648b-8a85-4abd-9659-9d61f8918841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the cleaned XML file using Spark\n",
    "df = read_with_spark(output_xml_path)\n",
    "\n",
    "# Show information about the DataFrame\n",
    "print(f\"Total number of records: {df.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c551906a-8eb4-48df-9d09-9d4bf94b52d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2cd14e-9afb-41d9-a47e-01e8e3eb0ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, lit\n",
    "import pandas as pd\n",
    "\n",
    "# Function to recursively extract field names with parent-child hierarchy\n",
    "def get_nested_fields(schema, prefix=\"\"):\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        field_name = f\"{prefix}.{field.name}\" if prefix else field.name\n",
    "        if hasattr(field.dataType, \"fields\"):  # Check if it's a struct (nested)\n",
    "            fields.extend(get_nested_fields(field.dataType, field_name))\n",
    "        else:\n",
    "            fields.append(field_name)\n",
    "    return fields\n",
    "\n",
    "# Get hierarchical field paths\n",
    "nested_fields = get_nested_fields(df.schema)\n",
    "\n",
    "# Calculate total number of records\n",
    "total_count = df.count()\n",
    "\n",
    "# Compute occurrence percentage of each column\n",
    "col_counts = (\n",
    "    df.select([(count(col(c)) / total_count * 100).alias(c) for c in nested_fields])\n",
    "    .toPandas()\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "col_counts.columns = [\"Tag\", \"Occurrence (%)\"]\n",
    "\n",
    "# Sort fields hierarchically\n",
    "col_counts[\"Tag\"] = col_counts[\"Tag\"].apply(lambda x: x.replace(\".\", \" → \"))  # Format for readability\n",
    "col_counts = col_counts.sort_values(\"Tag\")\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "hierarchical_df = spark.createDataFrame(col_counts)\n",
    "\n",
    "# Show the result\n",
    "hierarchical_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae1e193-fd57-401b-a83c-689727a779cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hierarchical_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bffe92-0445-45f5-b42b-f34fb09e9e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Calculate total number of records\n",
    "total_count = df.count()\n",
    "\n",
    "# Get hierarchical field paths\n",
    "nested_fields = get_nested_fields(df.schema)\n",
    "\n",
    "# Compute occurrence percentage of each column\n",
    "col_counts = (\n",
    "    df.select([(count(col(c)) / total_count * 100).alias(c) for c in nested_fields])\n",
    "    .toPandas()\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "col_counts.columns = [\"Tag\", \"Occurrence (%)\"]\n",
    "\n",
    "# Select columns with more than 80% occurrence\n",
    "selected_tags = col_counts[col_counts[\"Occurrence (%)\"] >= 80][\"Tag\"].tolist()\n",
    "\n",
    "# Replace back the formatted tags (in case needed)\n",
    "selected_columns = [tag.replace(\" → \", \".\") for tag in selected_tags]\n",
    "\n",
    "# Select these columns from the original DataFrame\n",
    "selected_df = df.select(*selected_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509544e3-2bb9-4270-904c-79dec9d388fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb1e3f9-292e-4e94-a20f-60aab3846985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6c913d-c8ca-4398-a175-740093ef2119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44f951d-b41d-4580-a30e-cb410bc47ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a mapping for the ambiguous column names\n",
    "column_mapping = {\n",
    "    # Abstract, claims, and description\n",
    "    '_VALUE': 'abstract_text',  # First occurrence\n",
    "    'claim': 'claims',\n",
    "    '_VALUE': 'description_text',  # Second occurrence\n",
    "    'heading': 'description_sections',\n",
    "    \n",
    "    # Application reference\n",
    "    'country': 'application_country',  # First occurrence\n",
    "    'date': 'application_date',  # First occurrence\n",
    "    'doc-number': 'application_number',  # First occurrence\n",
    "    \n",
    "    # CPC classifications\n",
    "    'classification-cpc': 'cpc_secondary',\n",
    "    'date': 'cpc_action_date',  # Second occurrence\n",
    "    'class': 'cpc_class',\n",
    "    'classification-data-source': 'cpc_data_source',\n",
    "    'classification-status': 'cpc_status',\n",
    "    'classification-value': 'cpc_value',\n",
    "    'date': 'cpc_version_date',  # Third occurrence\n",
    "    'country': 'cpc_office_country',  # Second occurrence\n",
    "    'main-group': 'cpc_main_group',\n",
    "    'scheme-origination-code': 'cpc_scheme_origin',\n",
    "    'section': 'cpc_section',\n",
    "    'subclass': 'cpc_subclass',\n",
    "    'subgroup': 'cpc_subgroup',\n",
    "    'symbol-position': 'cpc_symbol_position',\n",
    "    \n",
    "    # IPCR classification\n",
    "    'classification-ipcr': 'ipc_classification',\n",
    "    \n",
    "    # Publication reference\n",
    "    'invention-title': 'invention_title',\n",
    "    'country': 'publication_country',  # Third occurrence\n",
    "    'date': 'publication_date',  # Fourth occurrence\n",
    "    'doc-number': 'publication_number',  # Second occurrence\n",
    "    'kind': 'publication_kind',\n",
    "    \n",
    "    # Other fields\n",
    "    'us-application-series-code': 'application_series_code',\n",
    "    'inventor': 'inventors',\n",
    "    'us-applicant': 'applicants'\n",
    "}\n",
    "\n",
    "# Since there are duplicate column names in selected_df.columns, \n",
    "# we need to create a list that maintains the order and handles duplicates\n",
    "new_column_names = [\n",
    "    'abstract_text',\n",
    "    'claims',\n",
    "    'description_text',\n",
    "    'description_sections',\n",
    "    'application_country',\n",
    "    'application_date',\n",
    "    'application_number',\n",
    "    'cpc_secondary',\n",
    "    'cpc_action_date',\n",
    "    'cpc_class',\n",
    "    'cpc_data_source',\n",
    "    'cpc_status',\n",
    "    'cpc_value',\n",
    "    'cpc_version_date',\n",
    "    'cpc_office_country',\n",
    "    'cpc_main_group',\n",
    "    'cpc_scheme_origin',\n",
    "    'cpc_section',\n",
    "    'cpc_subclass',\n",
    "    'cpc_subgroup',\n",
    "    'cpc_symbol_position',\n",
    "    'ipc_classification',\n",
    "    'invention_title',\n",
    "    'publication_country',\n",
    "    'publication_date',\n",
    "    'publication_number',\n",
    "    'publication_kind',\n",
    "    'application_series_code',\n",
    "    'inventors',\n",
    "    'applicants'\n",
    "]\n",
    "\n",
    "# Create the renamed DataFrame\n",
    "renamed_df = selected_df.toDF(*new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f2f3c3-f74a-4a36-890c-89d9f097dea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "renamed_df.limit(20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c109c13c-8420-4a7c-835d-1d033bcb41ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "renamed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318e463b-4f99-4a01-bdd4-35a0c7e2c9bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat_ws, array_join, from_unixtime, to_date\n",
    "from pyspark.sql.functions import regexp_replace, lpad, when, length, expr\n",
    "\n",
    "def clean_date(col_obj):\n",
    "    \"\"\"Clean and format date strings from various formats to standard date\"\"\"\n",
    "    return to_date(regexp_replace(col_obj.cast(\"string\"), r\"[\\[\\]]\", \"\"), \"yyyyMMdd\")\n",
    "\n",
    "# Create helper functions for CPC component formatting\n",
    "def format_class(class_col):\n",
    "    \"\"\"Ensure class is exactly 2 digits\"\"\"\n",
    "    return lpad(class_col.cast(\"string\"), 2, \"0\")\n",
    "\n",
    "def format_group(group_col):\n",
    "    \"\"\"Format group (1-4 digits)\"\"\"\n",
    "    return when(group_col.isNotNull(), group_col.cast(\"string\")).otherwise(lit(\"\"))\n",
    "\n",
    "def format_subgroup(subgroup_col):\n",
    "    \"\"\"Ensure subgroup has at least 2 digits\"\"\"\n",
    "    return when(length(subgroup_col.cast(\"string\")) < 2, \n",
    "                lpad(subgroup_col.cast(\"string\"), 2, \"0\")\n",
    "            ).otherwise(subgroup_col.cast(\"string\"))\n",
    "\n",
    "# Improved patent dataframe with properly formatted CPC\n",
    "patent_df = renamed_df.select(\n",
    "    col(\"invention_title\"),\n",
    "    array_join(col(\"abstract_text\"), \" \").alias(\"abstract\"),\n",
    "\n",
    "    array_join(col(\"description_text\"), \" \").alias(\"description\"),\n",
    "    col(\"description_sections\"),\n",
    "    \n",
    "    # Additional classification\n",
    "    col(\"ipc_classification\"),\n",
    "    \n",
    "    # CPC metadata\n",
    "    clean_date(col(\"cpc_action_date\")).alias(\"cpc_action_date\"),\n",
    "    col(\"cpc_data_source\"),\n",
    "    col(\"cpc_status\"),\n",
    "    col(\"cpc_value\"),\n",
    "    clean_date(col(\"cpc_version_date\")).alias(\"cpc_version_date\"),\n",
    "    col(\"cpc_office_country\"),\n",
    "    col(\"cpc_scheme_origin\"),\n",
    "    col(\"cpc_symbol_position\"),\n",
    "    \n",
    "    # Publication reference\n",
    "    col(\"publication_country\"),\n",
    "    clean_date(col(\"publication_date\")).alias(\"publication_date\"),\n",
    "    col(\"publication_number\"),\n",
    "    col(\"publication_kind\"),\n",
    "    \n",
    "    # Application reference\n",
    "    col(\"application_country\"),\n",
    "    clean_date(col(\"application_date\")).alias(\"application_date\"),\n",
    "    col(\"application_number\"),\n",
    "    col(\"application_series_code\"),\n",
    "    \n",
    "    # CPC components individually for later analysis\n",
    "    col(\"cpc_section\").alias(\"cpc_section\"),\n",
    "    format_class(col(\"cpc_class\")).alias(\"cpc_class\"),\n",
    "    col(\"cpc_subclass\").alias(\"cpc_subclass\"),\n",
    "    format_group(col(\"cpc_main_group\")).alias(\"cpc_group\"),\n",
    "    format_subgroup(col(\"cpc_subgroup\")).alias(\"cpc_subgroup\"),\n",
    "    \n",
    "    # Full CPC code with proper structure (e.g., A01B33/00)\n",
    "    concat_ws(\"\", \n",
    "        col(\"cpc_section\"),                                    # A\n",
    "        format_class(col(\"cpc_class\")),                       # 01\n",
    "        col(\"cpc_subclass\"),                                  # B\n",
    "        format_group(col(\"cpc_main_group\")),                  # 33\n",
    "        lit(\"/\"),                                             # /\n",
    "        format_subgroup(col(\"cpc_subgroup\"))                  # 00\n",
    "    ).alias(\"cpc_main\"),\n",
    "    \n",
    "    # Hierarchical CPC code for tiered analysis (section, class, subclass)\n",
    "    concat_ws(\"\", \n",
    "        col(\"cpc_section\"),                                  \n",
    "        format_class(col(\"cpc_class\"))                      \n",
    "    ).alias(\"cpc_class_level\"),\n",
    "    \n",
    "    concat_ws(\"\", \n",
    "        col(\"cpc_section\"),                                  \n",
    "        format_class(col(\"cpc_class\")),                     \n",
    "        col(\"cpc_subclass\")                                 \n",
    "    ).alias(\"cpc_subclass_level\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff439ea-832b-49f6-9de3-70feb1107317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "patent_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb71c08-6e6d-42ef-b9c4-15462d458ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "# Inventors\n",
    "inventors_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"inventors\")).alias(\"inventor\")\n",
    ").select(\n",
    "    col(\"publication_number\"),\n",
    "    col(\"inventor.addressbook.first-name\").alias(\"first_name\"),\n",
    "    col(\"inventor.addressbook.last-name\").alias(\"last_name\"),\n",
    "    col(\"inventor.addressbook.address.city\").alias(\"city\"),\n",
    "    col(\"inventor.addressbook.address.state\").alias(\"state\"),\n",
    "    col(\"inventor.addressbook.address.country\").alias(\"country\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f31a600e-381f-463b-8b2d-f532221ec334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inventors_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11ccdfa-03c6-435f-9308-a2f57d3fd088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# US Applicants\n",
    "applicants_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"applicants\")).alias(\"applicant\")\n",
    ").select(\n",
    "    col(\"publication_number\"),\n",
    "    col(\"applicant.addressbook.first-name\").alias(\"first_name\"),\n",
    "    col(\"applicant.addressbook.last-name\").alias(\"last_name\"),\n",
    "    col(\"applicant.addressbook.orgname\").alias(\"organization_name\"),\n",
    "    col(\"applicant.addressbook.address.city\").alias(\"city\"),\n",
    "    col(\"applicant.addressbook.address.state\").alias(\"state\"),\n",
    "    col(\"applicant.addressbook.address.country\").alias(\"country\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b453491b-ad0d-41fc-be00-f3218107f5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "applicants_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e2ecd8-aca8-48eb-86c5-ba580e53fca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, explode_outer, collect_list, array_join, lit, when, flatten, posexplode\n",
    "\n",
    "# Extract claims\n",
    "claims_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"claims\")).alias(\"claim\")\n",
    ")\n",
    "\n",
    "# Function to recursively extract all _VALUE elements\n",
    "def extract_claim_text(claims_df):\n",
    "    # First level: extract the claim-text array from each claim\n",
    "    level1_df = claims_df.select(\n",
    "        \"publication_number\", \n",
    "        explode_outer(col(\"claim.claim-text\")).alias(\"claim_text_obj\")\n",
    "    )\n",
    "    \n",
    "    # Extract _VALUE arrays directly\n",
    "    level2_df = level1_df.select(\n",
    "        \"publication_number\",\n",
    "        col(\"claim_text_obj._VALUE\").alias(\"value_array\")\n",
    "    )\n",
    "    \n",
    "    # Handle nested claim-text if present (for deeply nested claims)\n",
    "    nested_claims_df = level1_df.filter(col(\"claim_text_obj.claim-text\").isNotNull())\n",
    "    \n",
    "    if nested_claims_df.count() > 0:\n",
    "        nested_df = nested_claims_df.select(\n",
    "            \"publication_number\",\n",
    "            explode_outer(col(\"claim_text_obj.claim-text\")).alias(\"nested_claim_text\")\n",
    "        ).select(\n",
    "            \"publication_number\",\n",
    "            col(\"nested_claim_text._VALUE\").alias(\"value_array\")\n",
    "        )\n",
    "        \n",
    "        # Union the direct values with nested values\n",
    "        return level2_df.union(nested_df)\n",
    "    else:\n",
    "        return level2_df\n",
    "\n",
    "# Extract all claim text values\n",
    "claim_values_df = extract_claim_text(claims_df)\n",
    "\n",
    "# Flatten and join arrays if needed\n",
    "claim_values_df = claim_values_df.withColumn(\n",
    "    \"claim_text\", \n",
    "    when(col(\"value_array\").isNotNull(),\n",
    "         array_join(col(\"value_array\"), \"# \")\n",
    "    ).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "# Group by publication_number to consolidate ALL claims into a single row\n",
    "final_claims_df = claim_values_df.groupBy(\"publication_number\").agg(\n",
    "    array_join(collect_list(\"claim_text\"), \"# \").alias(\"all_claims_text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aac0a8b-29a5-4295-a607-013c93e561f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_claims_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3f4891-b613-4e2b-ace2-0e4c2bc8f2d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "renamed_df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886d9f65-5669-4157-b5a0-ee1abc6ca276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat_ws, array_join, from_unixtime, to_date\n",
    "from pyspark.sql.functions import regexp_replace, lpad, when, length, expr, explode_outer\n",
    "from pyspark.sql.functions import collect_list, struct, array\n",
    "\n",
    "def clean_date(col_obj):\n",
    "    \"\"\"Clean and format date strings from various formats to standard date\"\"\"\n",
    "    return to_date(regexp_replace(col_obj.cast(\"string\"), r\"[\\[\\]]\", \"\"), \"yyyyMMdd\")\n",
    "\n",
    "# Create helper functions for classification component formatting\n",
    "def format_class(class_col):\n",
    "    \"\"\"Ensure class is exactly 2 digits\"\"\"\n",
    "    return lpad(class_col.cast(\"string\"), 2, \"0\")\n",
    "\n",
    "def format_group(group_col):\n",
    "    \"\"\"Format group (1-4 digits)\"\"\"\n",
    "    return when(group_col.isNotNull(), group_col.cast(\"string\")).otherwise(lit(\"\"))\n",
    "\n",
    "def format_subgroup(subgroup_col):\n",
    "    \"\"\"Ensure subgroup has at least 2 digits\"\"\"\n",
    "    return when(length(subgroup_col.cast(\"string\")) < 2, \n",
    "                lpad(subgroup_col.cast(\"string\"), 2, \"0\")\n",
    "            ).otherwise(subgroup_col.cast(\"string\"))\n",
    "\n",
    "# Function to format both CPC and IPC codes with the same structure\n",
    "def format_patent_class(section_col, class_col, subclass_col, main_group_col, subgroup_col):\n",
    "    \"\"\"Create standardized classification code string\"\"\"\n",
    "    return concat_ws(\"\", \n",
    "        section_col,                                    \n",
    "        format_class(class_col),                      \n",
    "        subclass_col,                                 \n",
    "        format_group(main_group_col),                 \n",
    "        lit(\"/\"),                                            \n",
    "        format_subgroup(subgroup_col)                 \n",
    "    )\n",
    "\n",
    "# Process IPC classifications first\n",
    "ipc_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"ipc_classification\")).alias(\"ipc\")\n",
    ").select(\n",
    "    col(\"publication_number\"),\n",
    "    col(\"ipc.section\").alias(\"ipc_section\"),\n",
    "    col(\"ipc.class\").alias(\"ipc_class\"),\n",
    "    col(\"ipc.subclass\").alias(\"ipc_subclass\"),\n",
    "    col(\"ipc.main-group\").alias(\"ipc_main_group\"),\n",
    "    col(\"ipc.subgroup\").alias(\"ipc_subgroup\"),\n",
    "    col(\"ipc.classification-value\").alias(\"ipc_value\"),\n",
    "    clean_date(col(\"ipc.action-date.date\")).alias(\"ipc_action_date\"),\n",
    "    col(\"ipc.classification-status\").alias(\"ipc_status\"),\n",
    "    col(\"ipc.classification-level\").alias(\"ipc_level\"),\n",
    "    col(\"ipc.classification-data-source\").alias(\"ipc_data_source\"),\n",
    "    col(\"ipc.generating-office.country\").alias(\"ipc_office_country\"),\n",
    "    clean_date(col(\"ipc.ipc-version-indicator.date\")).alias(\"ipc_version_date\"),\n",
    "    col(\"ipc.symbol-position\").alias(\"ipc_symbol_position\")\n",
    ")\n",
    "\n",
    "# Add formatted IPC code\n",
    "ipc_df = ipc_df.withColumn(\n",
    "    \"ipc_code\",\n",
    "    format_patent_class(\n",
    "        col(\"ipc_section\"),\n",
    "        col(\"ipc_class\"),\n",
    "        col(\"ipc_subclass\"),\n",
    "        col(\"ipc_main_group\"),\n",
    "        col(\"ipc_subgroup\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Group IPC information by publication_number\n",
    "ipc_grouped_df = ipc_df.groupBy(\"publication_number\").agg(\n",
    "    collect_list(\"ipc_code\").alias(\"ipc_codes\"),\n",
    "    collect_list(\n",
    "        struct(\n",
    "            \"ipc_code\", \"ipc_section\", \"ipc_class\", \"ipc_subclass\",\n",
    "            \"ipc_main_group\", \"ipc_subgroup\", \"ipc_value\", \"ipc_action_date\",\n",
    "            \"ipc_status\", \"ipc_level\", \"ipc_data_source\"\n",
    "        )\n",
    "    ).alias(\"ipc_details\")\n",
    ")\n",
    "\n",
    "# Extract inventors and applicants\n",
    "inventors_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"inventors\")).alias(\"inventor\")\n",
    ").select(\n",
    "    col(\"publication_number\"),\n",
    "    col(\"inventor.addressbook.first-name\").alias(\"inventor_first_name\"),\n",
    "    col(\"inventor.addressbook.last-name\").alias(\"inventor_last_name\"),\n",
    "    concat_ws(\" \", \n",
    "        col(\"inventor.addressbook.first-name\"), \n",
    "        col(\"inventor.addressbook.last-name\")\n",
    "    ).alias(\"inventor_name\"),\n",
    "    col(\"inventor.addressbook.address.city\").alias(\"inventor_city\"),\n",
    "    col(\"inventor.addressbook.address.state\").alias(\"inventor_state\"),\n",
    "    col(\"inventor.addressbook.address.country\").alias(\"inventor_country\")\n",
    ")\n",
    "\n",
    "# Group inventors\n",
    "inventors_grouped_df = inventors_df.groupBy(\"publication_number\").agg(\n",
    "    collect_list(\"inventor_name\").alias(\"inventor_names\"),\n",
    "    collect_list(\n",
    "        struct(\n",
    "            \"inventor_name\", \"inventor_city\", \"inventor_state\", \"inventor_country\"\n",
    "        )\n",
    "    ).alias(\"inventor_details\")\n",
    ")\n",
    "\n",
    "# Extract applicants\n",
    "applicants_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"applicants\")).alias(\"applicant\")\n",
    ").select(\n",
    "    col(\"publication_number\"),\n",
    "    col(\"applicant.addressbook.first-name\").alias(\"applicant_first_name\"),\n",
    "    col(\"applicant.addressbook.last-name\").alias(\"applicant_last_name\"),\n",
    "    col(\"applicant.addressbook.orgname\").alias(\"applicant_orgname\"),\n",
    "    when(col(\"applicant.addressbook.orgname\").isNotNull(), \n",
    "         col(\"applicant.addressbook.orgname\"))\n",
    "    .otherwise(\n",
    "        concat_ws(\" \", \n",
    "            col(\"applicant.addressbook.first-name\"),\n",
    "            col(\"applicant.addressbook.last-name\")\n",
    "        )\n",
    "    ).alias(\"applicant_name\"),\n",
    "    col(\"applicant.addressbook.address.city\").alias(\"applicant_city\"),\n",
    "    col(\"applicant.addressbook.address.state\").alias(\"applicant_state\"),\n",
    "    col(\"applicant.addressbook.address.country\").alias(\"applicant_country\")\n",
    ")\n",
    "\n",
    "# Extract US applicants\n",
    "us_applicants_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"applicants\")).alias(\"applicant\")\n",
    ").select(\n",
    "    col(\"publication_number\"),\n",
    "    col(\"applicant.addressbook.first-name\").alias(\"applicant_first_name\"),\n",
    "    col(\"applicant.addressbook.last-name\").alias(\"applicant_last_name\"),\n",
    "    col(\"applicant.addressbook.orgname\").alias(\"applicant_orgname\"),\n",
    "    when(col(\"applicant.addressbook.orgname\").isNotNull(), \n",
    "         col(\"applicant.addressbook.orgname\"))\n",
    "    .otherwise(\n",
    "        concat_ws(\" \", \n",
    "            col(\"applicant.addressbook.first-name\"),\n",
    "            col(\"applicant.addressbook.last-name\")\n",
    "        )\n",
    "    ).alias(\"applicant_name\"),\n",
    "    col(\"applicant.addressbook.address.city\").alias(\"applicant_city\"),\n",
    "    col(\"applicant.addressbook.address.state\").alias(\"applicant_state\"),\n",
    "    col(\"applicant.addressbook.address.country\").alias(\"applicant_country\")\n",
    ").filter(col(\"applicant_country\") == \"US\")  # Filter for US applicants\n",
    "\n",
    "# Group US applicants\n",
    "applicants_grouped_df = applicants_df.groupBy(\"publication_number\").agg(\n",
    "    collect_list(\"applicant_name\").alias(\"us_applicant_names\"),\n",
    "    collect_list(\n",
    "        struct(\n",
    "            \"applicant_name\", \"applicant_city\", \"applicant_state\", \"applicant_country\"\n",
    "        )\n",
    "    ).alias(\"us_applicant_details\")\n",
    ")\n",
    "\n",
    "# Extract claims\n",
    "claims_df = renamed_df.select(\n",
    "    col(\"publication_number\"),\n",
    "    explode_outer(col(\"claims\")).alias(\"claim\")\n",
    ")\n",
    "\n",
    "# Function to extract claim text values recursively\n",
    "def extract_claim_text(claims_df):\n",
    "    # First level extraction\n",
    "    level1_df = claims_df.select(\n",
    "        \"publication_number\", \n",
    "        explode_outer(col(\"claim.claim-text\")).alias(\"claim_text_obj\")\n",
    "    )\n",
    "    \n",
    "    # Extract direct _VALUE arrays\n",
    "    level2_df = level1_df.select(\n",
    "        \"publication_number\",\n",
    "        col(\"claim_text_obj._VALUE\").alias(\"value_array\")\n",
    "    )\n",
    "    \n",
    "    # Handle nested claim-text if present\n",
    "    nested_claims_df = level1_df.filter(col(\"claim_text_obj.claim-text\").isNotNull())\n",
    "    \n",
    "    if nested_claims_df.count() > 0:\n",
    "        nested_df = nested_claims_df.select(\n",
    "            \"publication_number\",\n",
    "            explode_outer(col(\"claim_text_obj.claim-text\")).alias(\"nested_claim_text\")\n",
    "        ).select(\n",
    "            \"publication_number\",\n",
    "            col(\"nested_claim_text._VALUE\").alias(\"value_array\")\n",
    "        )\n",
    "        \n",
    "        # Union direct and nested values\n",
    "        return level2_df.union(nested_df)\n",
    "    else:\n",
    "        return level2_df\n",
    "\n",
    "# Process claims\n",
    "claim_values_df = extract_claim_text(claims_df)\n",
    "claim_values_df = claim_values_df.withColumn(\n",
    "    \"claim_text\", \n",
    "    when(col(\"value_array\").isNotNull(),\n",
    "         array_join(col(\"value_array\"), \"# \")\n",
    "    ).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "# Aggregate claims by publication_number\n",
    "claims_grouped_df = claim_values_df.groupBy(\"publication_number\").agg(\n",
    "    array_join(collect_list(\"claim_text\"), \"# \").alias(\"all_claims_text\")\n",
    ")\n",
    "\n",
    "# Improved patent dataframe with properly formatted CPC and additional data\n",
    "patent_df = renamed_df.select(\n",
    "    # Basic patent information\n",
    "    col(\"publication_number\"),\n",
    "    col(\"invention_title\"),\n",
    "    array_join(col(\"abstract_text\"), \" \").alias(\"abstract\"),\n",
    "    array_join(col(\"description_text\"), \" \").alias(\"description\"),\n",
    "\n",
    "    # Publication reference\n",
    "    col(\"publication_country\"),\n",
    "    clean_date(col(\"publication_date\")).alias(\"publication_date\"),\n",
    "    col(\"publication_kind\"),\n",
    "    \n",
    "    # Application reference\n",
    "    col(\"application_country\"),\n",
    "    clean_date(col(\"application_date\")).alias(\"application_date\"),\n",
    "    col(\"application_number\"),\n",
    "    col(\"application_series_code\"),\n",
    "    \n",
    "    # CPC components individually for later analysis\n",
    "    col(\"cpc_section\").alias(\"cpc_section\"),\n",
    "    format_class(col(\"cpc_class\")).alias(\"cpc_class\"),\n",
    "    col(\"cpc_subclass\").alias(\"cpc_subclass\"),\n",
    "    format_group(col(\"cpc_main_group\")).alias(\"cpc_group\"),\n",
    "    format_subgroup(col(\"cpc_subgroup\")).alias(\"cpc_subgroup\"),\n",
    "    \n",
    "    # CPC metadata\n",
    "    clean_date(col(\"cpc_action_date\")).alias(\"cpc_action_date\"),\n",
    "    col(\"cpc_data_source\"),\n",
    "    col(\"cpc_status\"),\n",
    "    col(\"cpc_value\"),\n",
    "    clean_date(col(\"cpc_version_date\")).alias(\"cpc_version_date\"),\n",
    "    col(\"cpc_office_country\"),\n",
    "    col(\"cpc_scheme_origin\"),\n",
    "    col(\"cpc_symbol_position\"),\n",
    "    \n",
    "    # Full CPC code with proper structure (e.g., A01B33/00)\n",
    "    format_patent_class(\n",
    "        col(\"cpc_section\"),\n",
    "        col(\"cpc_class\"),\n",
    "        col(\"cpc_subclass\"),\n",
    "        col(\"cpc_main_group\"),\n",
    "        col(\"cpc_subgroup\")\n",
    "    ).alias(\"cpc_main\"),\n",
    "    \n",
    "    # Hierarchical CPC code for tiered analysis\n",
    "    concat_ws(\"\", \n",
    "        col(\"cpc_section\"),                                  \n",
    "        format_class(col(\"cpc_class\"))                      \n",
    "    ).alias(\"cpc_class_level\"),\n",
    "    \n",
    "    concat_ws(\"\", \n",
    "        col(\"cpc_section\"),                                  \n",
    "        format_class(col(\"cpc_class\")),                     \n",
    "        col(\"cpc_subclass\")                                 \n",
    "    ).alias(\"cpc_subclass_level\")\n",
    ")\n",
    "\n",
    "# Join all the components\n",
    "complete_patent_df = patent_df.join(\n",
    "    claims_grouped_df, on=\"publication_number\", how=\"left\"\n",
    ").join(\n",
    "    ipc_grouped_df, on=\"publication_number\", how=\"left\"\n",
    ").join(\n",
    "    inventors_grouped_df, on=\"publication_number\", how=\"left\"\n",
    ").join(\n",
    "    applicants_grouped_df, on=\"publication_number\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70dd8287-c088-440c-8c38-a932729abb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "complete_patent_df.limit(10).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-03-14 18:49:27",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
