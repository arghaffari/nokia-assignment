{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d5edf7-fc32-4e3a-b0d3-1c6aa54b87d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `nokia-assginment-catalog`;\n",
    "-- drop schema patent_data cascade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a709b855-176b-4367-bb14-b1c4a2f2b424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to create a widget to control schema dropping\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"drop_patent_data_schema\", \"false\", [\"true\", \"false\"], \"Drop schema patent_data cascade\")\n",
    "    drop_patent_data_schema = dbutils.widgets.get(\"drop_patent_data_schema\") == \"true\"\n",
    "except:\n",
    "    # Default to not dropping schema in job mode\n",
    "    drop_patent_data_schema = False\n",
    "\n",
    "print(f\"Drop patent_data schema setting: {drop_patent_data_schema}\")\n",
    "\n",
    "# Execute SQL to drop schema if requested\n",
    "if drop_patent_data_schema:\n",
    "    try:\n",
    "        print(\"Dropping schema patent_data cascade...\")\n",
    "        spark.sql(\"DROP SCHEMA IF EXISTS patent_data CASCADE\")\n",
    "        print(\"Schema patent_data successfully dropped\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping schema: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c57a3c4-2371-4d2c-b4aa-418b405a7139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 -U --quiet\n",
    "%pip install lxml -U --quiet\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644e1d40-2429-4e30-85b7-d33e0d02c7fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "import re\n",
    "import os\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "# Define the HTML tags to remove (keep their content)\n",
    "html_tags = {\n",
    "    \"b\", \"i\", \"u\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \n",
    "    \"span\", \"div\", \"br\", \"strong\", \"em\", \"sub\", \"sup\", \"drawings\", \n",
    "    \"figure\", \"img\", \"ol\", \"ul\", \"li\", \"ol\", \"table\", \"tr\", \"td\", \"o\", \"us-math\", \"us-chemistry\", \n",
    "    \"us-sequence-list-doc\", \"sequence-list\", \"us-claim-statement\",\n",
    "    \"th\", \"tbody\", \"thead\", \"tfoot\", \"figref\", \"description-of-drawings\", \n",
    "    \"summary-of-invention\", \"brief-description-of-drawings\", \"figure\", \"claim-ref\"\n",
    "}\n",
    "\n",
    "def clean_html_tags_lxml(xml_content):\n",
    "    \"\"\"Remove only the specified HTML tags from the html_tags set using exact matching\"\"\"\n",
    "    try:\n",
    "        # Log the XML content size before cleaning\n",
    "        print(f\"XML content before cleaning: {len(xml_content)} bytes\")\n",
    "        \n",
    "        # Only remove tags that match exactly our html_tags set\n",
    "        for tag in html_tags:\n",
    "            # Create patterns that exactly match our tags\n",
    "            # Remove opening tags with attributes but keep content\n",
    "            xml_content = re.sub(fr'<{tag}(\\s+[^>]*)?>', '', xml_content)\n",
    "            # Remove closing tags\n",
    "            xml_content = re.sub(fr'</{tag}>', '', xml_content)\n",
    "            # Handle processing instruction format\n",
    "            xml_content = re.sub(fr'<\\?{tag}(\\s+[^>]*)?\\?>', '', xml_content)\n",
    "        \n",
    "        # Log the XML content size after cleaning\n",
    "        print(f\"XML content after cleaning: {len(xml_content)} bytes\")\n",
    "        \n",
    "        # Validate that the cleaned XML still has the required structure\n",
    "        if \"<us-patent-application\" not in xml_content or \"</us-patent-application>\" not in xml_content:\n",
    "            print(\"WARNING: Cleaned XML may be missing required us-patent-application tags!\")\n",
    "        \n",
    "        return xml_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in XML cleaning: {str(e)}\")\n",
    "        # Return original content if cleaning fails\n",
    "        return xml_content\n",
    "\n",
    "def process_xml_file(input_path, output_path):\n",
    "    \"\"\"Process the XML file to remove HTML tags and save to a new file\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Read the entire file\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Find the XML declaration if present and preserve it\n",
    "    xml_declaration = \"\"\n",
    "    xml_declaration_match = re.match(r'(<\\?xml[^>]*\\?>)', content)\n",
    "    if xml_declaration_match:\n",
    "        xml_declaration = xml_declaration_match.group(1)\n",
    "    \n",
    "    # Find the root tag\n",
    "    root_start = content.find('<us-patent-applications>')\n",
    "    if root_start == -1:\n",
    "        root_start = content.find('<us-patent-application')\n",
    "    \n",
    "    # Write the start of the file\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        # Write the XML declaration first\n",
    "        if xml_declaration:\n",
    "            outfile.write(xml_declaration + '\\n')\n",
    "        \n",
    "        # Write root tag if it exists\n",
    "        if '<us-patent-applications>' in content:\n",
    "            outfile.write('<us-patent-applications>\\n')\n",
    "        \n",
    "        # Process each patent application separately\n",
    "        start_tag = '<us-patent-application'\n",
    "        end_tag = '</us-patent-application>'\n",
    "        \n",
    "        pos = content.find(start_tag)\n",
    "        applications_processed = 0\n",
    "        \n",
    "        while pos != -1:\n",
    "            # Find the end of this patent application\n",
    "            end_pos = content.find(end_tag, pos) + len(end_tag)\n",
    "            if end_pos == -1:\n",
    "                break\n",
    "            \n",
    "            # Extract and clean this patent application\n",
    "            patent_xml = content[pos:end_pos]\n",
    "            patent_size = len(patent_xml)\n",
    "            print(f\"Processing application {applications_processed+1} at position {pos}-{end_pos}\")\n",
    "            print(f\"Patent XML size: {patent_size} bytes\")\n",
    "            \n",
    "            # Clean the patent XML\n",
    "            cleaned_patent = clean_html_tags_lxml(patent_xml)\n",
    "            cleaned_size = len(cleaned_patent)\n",
    "            print(f\"Cleaned patent size: {cleaned_size} bytes\")\n",
    "            print(f\"Size reduction: {patent_size - cleaned_size} bytes ({(patent_size - cleaned_size) / patent_size * 100:.2f}%)\")\n",
    "            \n",
    "            outfile.write(cleaned_patent + '\\n')\n",
    "            applications_processed += 1\n",
    "            \n",
    "            # Move to next patent application\n",
    "            pos = content.find(start_tag, end_pos)\n",
    "        \n",
    "        # Close the root element if it exists\n",
    "        if '<us-patent-applications>' in content:\n",
    "            outfile.write('</us-patent-applications>')\n",
    "        \n",
    "        print(f\"Processed {applications_processed} patent applications\")\n",
    "        \n",
    "        # Verify output file\n",
    "        if os.path.exists(output_path):\n",
    "            outfile_size = os.path.getsize(output_path)\n",
    "            print(f\"Output file size: {outfile_size} bytes\")\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session with XML reader configurations\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"XML Patent Processor\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.15.0\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.sql.legacy.allowUntypedScalaUDF\", \"true\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def check_file_already_processed(spark, file_name, processed_files_dir):\n",
    "    \"\"\"Check if a file has already been processed by looking for its output directory\"\"\"\n",
    "    file_base_name = file_name.replace('.xml', '')\n",
    "    output_path = f\"{processed_files_dir}/{file_base_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if directory exists and has parquet files\n",
    "        files = dbutils.fs.ls(output_path)\n",
    "        parquet_files = [f for f in files if f.name.endswith('.parquet')]\n",
    "        if len(parquet_files) > 0:\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        # Directory doesn't exist\n",
    "        return False\n",
    "\n",
    "def bronze_layer_processing():\n",
    "    \"\"\"Process XML files using optimized parallel processing\"\"\"\n",
    "    print(\"Starting bronze layer processing\")\n",
    "    \n",
    "    # Try to create a widget to control reprocessing (only works in interactive mode)\n",
    "    try:\n",
    "        dbutils.widgets.dropdown(\"force_reprocess\", \"false\", [\"true\", \"false\"], \"Force Reprocessing\")\n",
    "        force_reprocess = dbutils.widgets.get(\"force_reprocess\") == \"true\"\n",
    "    except:\n",
    "        # Default to incremental processing in job mode\n",
    "        force_reprocess = False\n",
    "    \n",
    "    print(f\"Force reprocessing mode: {force_reprocess}\")\n",
    "    \n",
    "    spark = initialize_spark()\n",
    "    \n",
    "    # Unity Catalog paths\n",
    "    input_path = \"/Volumes/nokia-assginment-catalog/assignment_data/xml_raw_data\"\n",
    "    bronze_path = \"/Volumes/nokia-assginment-catalog/bronze\"\n",
    "    temp_processing_dir = \"/tmp/xml_processing/\"\n",
    "    checkpoint_location = \"/Volumes/nokia-assginment-catalog/checkpoints/checkpoints_data/xml_autoloader/\"\n",
    "    \n",
    "    # Use a clear path for Parquet output\n",
    "    parquet_output_path = f\"{bronze_path}/raw_data\"\n",
    "    \n",
    "    # Clear temp directory if it exists\n",
    "    os.system(f\"rm -rf {temp_processing_dir}\")\n",
    "    print(\"Cleared temp processing directory\")\n",
    "    \n",
    "    try:\n",
    "        # Check input directory to confirm files exist\n",
    "        print(f\"Checking input directory: {input_path=}\")\n",
    "        try:\n",
    "            input_files = dbutils.fs.ls(input_path)\n",
    "            xml_files = [f for f in input_files if f.name.endswith('.xml')]\n",
    "            print(f\"Found {len(xml_files)} XML files in input directory\")\n",
    "            for xml_file in xml_files[:5]:  # List first 5 files\n",
    "                print(f\"  {xml_file.name} ({xml_file.size} bytes)\")\n",
    "            \n",
    "            if len(xml_files) == 0:\n",
    "                print(\"ERROR: No XML files found in input directory!\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing input files: {str(e)}\")\n",
    "        \n",
    "        # Handle checkpoint directory based on force_reprocess flag\n",
    "        if force_reprocess:\n",
    "            print(f\"Force reprocessing requested, clearing checkpoint location: {checkpoint_location}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(checkpoint_location, True)\n",
    "                print(\"Checkpoint directory cleared\")\n",
    "            except:\n",
    "                print(\"No checkpoint directory to clear\")\n",
    "            \n",
    "            dbutils.fs.mkdirs(checkpoint_location)\n",
    "            print(\"Created new checkpoint directory\")\n",
    "        else:\n",
    "            print(\"Using existing checkpoint directory for incremental processing\")\n",
    "            # Just ensure the directory exists\n",
    "            try:\n",
    "                dbutils.fs.ls(checkpoint_location)\n",
    "                print(\"Checkpoint directory exists\")\n",
    "            except:\n",
    "                dbutils.fs.mkdirs(checkpoint_location)\n",
    "                print(\"Created checkpoint directory\")\n",
    "        \n",
    "        # Create necessary directories\n",
    "        os.makedirs(os.path.join(temp_processing_dir, \"input\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(temp_processing_dir, \"cleaned\"), exist_ok=True)\n",
    "        \n",
    "        # Handle output directory based on force_reprocess flag\n",
    "        if force_reprocess:\n",
    "            print(f\"Force reprocessing requested, clearing output directory: {parquet_output_path}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(parquet_output_path, True)\n",
    "                print(\"Removed existing Parquet directory\")\n",
    "            except:\n",
    "                print(\"No existing Parquet directory to remove\")\n",
    "            \n",
    "            dbutils.fs.mkdirs(parquet_output_path)\n",
    "            print(\"Created Parquet output directory\")\n",
    "        else:\n",
    "            print(\"Using existing output directory for incremental files\")\n",
    "            # Just ensure the directory exists\n",
    "            try:\n",
    "                dbutils.fs.ls(parquet_output_path)\n",
    "                print(\"Parquet output directory exists\")\n",
    "            except:\n",
    "                dbutils.fs.mkdirs(parquet_output_path)\n",
    "                print(\"Created Parquet output directory\")\n",
    "        \n",
    "        # Check if bronze cleaned_raw_xml_data directory exists\n",
    "        try:\n",
    "            dbutils.fs.ls(f\"{bronze_path}/cleaned_raw_xml_data\")\n",
    "            print(\"Bronze cleaned directory exists\")\n",
    "        except:\n",
    "            print(\"Creating bronze cleaned directory\")\n",
    "            dbutils.fs.mkdirs(f\"{bronze_path}/cleaned_raw_xml_data\")\n",
    "        \n",
    "        # Set up Auto Loader in batch mode\n",
    "        print(\"Setting up Auto Loader\")\n",
    "        autoloader_df = (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "            .option(\"cloudFiles.schemaLocation\", checkpoint_location)\n",
    "            .option(\"pathGlobFilter\", \"*.xml\")\n",
    "            .load(input_path)\n",
    "        )\n",
    "        \n",
    "        # Define a function to process each file in the batch\n",
    "        def process_batch(batch_df, batch_id):\n",
    "            # Process only if there are files in this batch\n",
    "            batch_count = batch_df.count()\n",
    "            print(f\"Processing batch {batch_id} with {batch_count} files\")\n",
    "            \n",
    "            if batch_count > 0:\n",
    "                # Process each file in the batch\n",
    "                file_list = batch_df.select(\"path\").collect()\n",
    "                \n",
    "                for file_index, file_row in enumerate(file_list):\n",
    "                    file_path = file_row.path\n",
    "                    file_name = os.path.basename(file_path)\n",
    "                    \n",
    "                    # Check if file was already processed (only in incremental mode)\n",
    "                    if not force_reprocess and check_file_already_processed(spark, file_name, parquet_output_path):\n",
    "                        print(f\"Skipping already processed file: {file_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Processing file {file_index+1}/{batch_count}: {file_name}\")\n",
    "                    \n",
    "                    # Local paths for processing\n",
    "                    local_input_path = os.path.join(temp_processing_dir, \"input\", file_name)\n",
    "                    local_output_path = os.path.join(temp_processing_dir, \"cleaned\", file_name)\n",
    "                    \n",
    "                    try:\n",
    "                        # Download file from Unity Catalog volume to local temp storage\n",
    "                        print(f\"Downloading file from {file_path}\")\n",
    "                        dbutils.fs.cp(file_path, f\"file:{local_input_path}\")\n",
    "                        \n",
    "                        # Clean the XML file\n",
    "                        print(f\"Processing {file_name}...\")\n",
    "                        process_xml_file(local_input_path, local_output_path)\n",
    "                        \n",
    "                        # Verify the cleaned file has content\n",
    "                        if os.path.exists(local_output_path) and os.path.getsize(local_output_path) > 0:\n",
    "                            print(f\"Cleaned file created successfully, size: {os.path.getsize(local_output_path)} bytes\")\n",
    "                        else:\n",
    "                            print(\"Warning: Cleaned file is empty or doesn't exist\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Upload cleaned file to bronze layer\n",
    "                        bronze_file_path = f\"{bronze_path}/cleaned_raw_xml_data/{file_name.replace('.xml', '_cleaned.xml')}\"\n",
    "                        print(f\"Uploading cleaned file to {bronze_file_path}\")\n",
    "                        dbutils.fs.cp(f\"file:{local_output_path}\", bronze_file_path)\n",
    "                        \n",
    "                        # Read the cleaned XML file using Spark\n",
    "                        print(\"Reading with Spark XML reader\")\n",
    "                        try:\n",
    "                            df = (\n",
    "                                spark.read.format(\"xml\")\n",
    "                                .option(\"rowTag\", \"us-patent-application\")\n",
    "                                .option(\"charset\", \"UTF-8\")\n",
    "                                .option(\"ignoreSurroundingSpaces\", \"true\")\n",
    "                                .option(\"mode\", \"PERMISSIVE\")\n",
    "                                .option(\"excludeAttribute\", \"true\")\n",
    "                                .option(\"includeMetadata\", \"true\")\n",
    "                                .option(\"valueTag\", \"_VALUE\")\n",
    "                                .load(bronze_file_path)\n",
    "                                .withColumn(\"source_file\", lit(file_name))\n",
    "                                .withColumn(\"ingestion_date\", current_timestamp())\n",
    "                            ).drop('_VALUE')\n",
    "                            \n",
    "                            row_count = df.count()\n",
    "                            print(f\"Dataframe created with {row_count} rows\")\n",
    "                            \n",
    "                            if row_count > 0:\n",
    "                                # Write to Parquet files with simplified options\n",
    "                                file_output_path = f\"{parquet_output_path}/{file_name.replace('.xml', '')}\"\n",
    "                                print(f\"Writing to Parquet files at {file_output_path}\")\n",
    "                                \n",
    "                                # Simple Parquet write\n",
    "                                (df.write\n",
    "                                   .format(\"parquet\")\n",
    "                                   .mode(\"overwrite\")\n",
    "                                   .save(file_output_path))\n",
    "                                \n",
    "                                print(f\"Added {row_count} records from {file_name} to Parquet output\")\n",
    "                            else:\n",
    "                                print(f\"WARNING: No records found in {file_name} after XML parsing\")\n",
    "                                \n",
    "                                # Try with different rowTag as a diagnostic\n",
    "                                try:\n",
    "                                    alt_df = spark.read.format(\"xml\") \\\n",
    "                                        .option(\"rowTag\", \"patent-application\") \\\n",
    "                                        .option(\"mode\", \"PERMISSIVE\") \\\n",
    "                                        .load(bronze_file_path)\n",
    "                                    alt_count = alt_df.count()\n",
    "                                    print(f\"Alternative rowTag found {alt_count} records\")\n",
    "                                except Exception as diag_error:\n",
    "                                    print(f\"Diagnostic error: {str(diag_error)}\")\n",
    "                        \n",
    "                        except Exception as spark_error:\n",
    "                            print(f\"Error processing with Spark: {str(spark_error)}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_name}: {str(e)}\")\n",
    "            else:\n",
    "                print(\"No files to process in this batch\")\n",
    "        \n",
    "        # Use Structured Streaming to run Auto Loader in batch mode\n",
    "        print(\"Starting Auto Loader stream\")\n",
    "        stream = (autoloader_df.writeStream\n",
    "            .foreachBatch(process_batch)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .trigger(once=True)  # Run once and stop\n",
    "            .start())\n",
    "        \n",
    "        print(\"Waiting for stream to complete\")\n",
    "        stream.awaitTermination()\n",
    "        print(\"Stream completed\")\n",
    "        \n",
    "        # Check if output was created\n",
    "        try:\n",
    "            files = dbutils.fs.ls(parquet_output_path)\n",
    "            print(f\"Found {len(files)} items in Parquet output directory\")\n",
    "            \n",
    "            total_parquet_files = 0\n",
    "            for item in files:\n",
    "                if item.name.endswith('.parquet'):\n",
    "                    total_parquet_files += 1\n",
    "                else:\n",
    "                    # Check subdirectories\n",
    "                    try:\n",
    "                        subfiles = dbutils.fs.ls(item.path)\n",
    "                        parquet_in_dir = [f for f in subfiles if f.name.endswith('.parquet')]\n",
    "                        total_parquet_files += len(parquet_in_dir)\n",
    "                        print(f\"  Directory {item.name}: {len(parquet_in_dir)} Parquet files\")\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            print(f\"Found a total of {total_parquet_files} Parquet files\")\n",
    "            if total_parquet_files == 0:\n",
    "                print(\"WARNING: No Parquet files were created!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking output directory: {str(e)}\")\n",
    "        \n",
    "        # Cleanup temp directory\n",
    "        os.system(f\"rm -rf {temp_processing_dir}\")\n",
    "        print(\"Cleaned up temporary directory\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bronze layer processing: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Exception traceback: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "# Execute the function\n",
    "bronze_layer_processing()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 899871592771263,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "source_to_bronze_ingest_clean_convert_xml_files",
   "widgets": {
    "drop_patent_data_schema": {
     "currentValue": "false",
     "nuid": "f20dfcc6-15f5-43d0-8120-fdb9f5f65ccb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Drop schema patent_data cascade",
      "name": "drop_patent_data_schema",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Drop schema patent_data cascade",
      "name": "drop_patent_data_schema",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "force_reprocess": {
     "currentValue": "false",
     "nuid": "ba658067-a31a-4ef3-b739-a4a2c18a2b41",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Force Reprocessing",
      "name": "force_reprocess",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Force Reprocessing",
      "name": "force_reprocess",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
