{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13252e74-6bd5-4891-8150-e782f9b530ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `nokia-assginment-catalog`;\n",
    "-- drop schema patent_data cascade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d826e286-eba3-42a3-9918-60c6dfb149df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to create a widget to control schema dropping\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"drop_patent_data_schema\", \"false\", [\"true\", \"false\"], \"Drop schema patent_data cascade\")\n",
    "    drop_patent_data_schema = dbutils.widgets.get(\"drop_patent_data_schema\") == \"true\"\n",
    "except:\n",
    "    # Default to not dropping schema in job mode\n",
    "    drop_patent_data_schema = False\n",
    "\n",
    "print(f\"Drop patent_data schema setting: {drop_patent_data_schema}\")\n",
    "\n",
    "# Execute SQL to drop schema if requested\n",
    "if drop_patent_data_schema:\n",
    "    try:\n",
    "        print(\"Dropping schema patent_data cascade...\")\n",
    "        spark.sql(\"DROP SCHEMA IF EXISTS patent_data CASCADE\")\n",
    "        print(\"Schema patent_data successfully dropped\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping schema: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea61ddb-95b8-4926-af97-589b3d023687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit, current_timestamp\n",
    "from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session with Delta Lake support\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Patent Silver Layer Processor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def get_nested_fields(schema, prefix=\"\", max_depth=None, current_depth=0):\n",
    "    \"\"\"Recursively extract field names with parent-child hierarchy up to max_depth\"\"\"\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        field_name = f\"{prefix}.{field.name}\" if prefix else field.name\n",
    "        if hasattr(field.dataType, \"fields\") and (max_depth is None or current_depth < max_depth):  # Check if it's a struct (nested)\n",
    "            fields.extend(get_nested_fields(field.dataType, field_name, max_depth, current_depth + 1))\n",
    "        else:\n",
    "            fields.append(field_name)\n",
    "    return fields\n",
    "\n",
    "def analyze_field_occurrence(df, max_depth=None):\n",
    "    \"\"\"Analyze field occurrence percentage and return hierarchical view\"\"\"\n",
    "    # Calculate total number of records\n",
    "    total_count = df.count()\n",
    "    \n",
    "    # Get hierarchical field paths\n",
    "    nested_fields = get_nested_fields(df.schema, max_depth=max_depth)\n",
    "    \n",
    "    # Compute occurrence percentage of each column\n",
    "    col_counts = (\n",
    "        df.select([(count(col(c)) / total_count * 100).alias(c) for c in nested_fields])\n",
    "        .toPandas()\n",
    "        .transpose()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Rename columns\n",
    "    col_counts.columns = [\"Tag\", \"Occurrence (%)\"]\n",
    "    \n",
    "    # Sort fields hierarchically\n",
    "    col_counts[\"Tag\"] = col_counts[\"Tag\"].apply(lambda x: x.replace(\".\", \" â†’ \"))  # Format for readability\n",
    "    col_counts = col_counts.sort_values(\"Occurrence (%)\", ascending=False)  # Sort by descending occurrence\n",
    "    \n",
    "    # Convert back to Spark DataFrame\n",
    "    spark = df.sparkSession\n",
    "    hierarchical_df = spark.createDataFrame(col_counts)\n",
    "    \n",
    "    return hierarchical_df, col_counts\n",
    "\n",
    "def transform_and_rename_fields(df, occurrence_threshold=80):\n",
    "    \"\"\"Transform the bronze dataframe into silver with selected fields and column renaming\"\"\"\n",
    "    # Calculate total number of records\n",
    "    total_count = df.count()\n",
    "    \n",
    "    # Get hierarchical field paths\n",
    "    nested_fields = get_nested_fields(df.schema)\n",
    "    \n",
    "    # Compute occurrence percentage of each column\n",
    "    col_counts = (\n",
    "        df.select([(count(col(c)) / total_count * 100).alias(c) for c in nested_fields])\n",
    "        .toPandas()\n",
    "        .transpose()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Rename columns\n",
    "    col_counts.columns = [\"Tag\", \"Occurrence (%)\"]\n",
    "    \n",
    "    # Select columns with more than threshold% occurrence\n",
    "    selected_tags = col_counts[col_counts[\"Occurrence (%)\"] >= occurrence_threshold][\"Tag\"].tolist()\n",
    "    \n",
    "    # Replace back the formatted tags\n",
    "    selected_columns = [tag for tag in selected_tags]\n",
    "    \n",
    "    # Detect all column names in the DataFrame\n",
    "    all_columns = df.columns\n",
    "    print(f\"Available columns: {all_columns}\")\n",
    "    \n",
    "    # Create a mapping for the ambiguous column names\n",
    "    column_mapping = {\n",
    "        # Abstract, claims, and description\n",
    "        '_VALUE': 'abstract_text',  # First occurrence\n",
    "        'claim': 'claims',\n",
    "        '_VALUE': 'description_text',  # Second occurrence\n",
    "        'heading': 'description_sections',\n",
    "        \n",
    "        # Application reference\n",
    "        'country': 'application_country',  # First occurrence\n",
    "        'date': 'application_date',  # First occurrence\n",
    "        'doc-number': 'application_number',  # First occurrence\n",
    "        \n",
    "        # CPC classifications\n",
    "        'classification-cpc': 'cpc_secondary',\n",
    "        'date': 'cpc_action_date',  # Second occurrence\n",
    "        'class': 'cpc_class',\n",
    "        'classification-data-source': 'cpc_data_source',\n",
    "        'classification-status': 'cpc_status',\n",
    "        'classification-value': 'cpc_value',\n",
    "        'date': 'cpc_version_date',  # Third occurrence\n",
    "        'country': 'cpc_office_country',  # Second occurrence\n",
    "        'main-group': 'cpc_main_group',\n",
    "        'scheme-origination-code': 'cpc_scheme_origin',\n",
    "        'section': 'cpc_section',\n",
    "        'subclass': 'cpc_subclass',\n",
    "        'subgroup': 'cpc_subgroup',\n",
    "        'symbol-position': 'cpc_symbol_position',\n",
    "        \n",
    "        # IPCR classification\n",
    "        'classification-ipcr': 'ipc_classification',\n",
    "        \n",
    "        # Publication reference\n",
    "        'invention-title': 'invention_title',\n",
    "        'country': 'publication_country',  # Third occurrence\n",
    "        'date': 'publication_date',  # Fourth occurrence\n",
    "        'doc-number': 'publication_number',  # Second occurrence\n",
    "        'kind': 'publication_kind',\n",
    "        \n",
    "        # Other fields\n",
    "        'us-application-series-code': 'application_series_code',\n",
    "        'inventor': 'inventors',\n",
    "        'us-applicant': 'applicants',\n",
    "        'source_file': 'source_file',\n",
    "        'ingestion_date': 'ingestion_date'\n",
    "    }\n",
    "    \n",
    "    # Since there are duplicate column names in selected_columns, \n",
    "    # we need to create a list that maintains the order and handles duplicates\n",
    "    new_column_names = [\n",
    "        'abstract_text',\n",
    "        'claims',\n",
    "        'description_text',\n",
    "        'description_sections',\n",
    "        'application_country',\n",
    "        'application_date',\n",
    "        'application_number',\n",
    "        'cpc_secondary',\n",
    "        'cpc_action_date',\n",
    "        'cpc_class',\n",
    "        'cpc_data_source',\n",
    "        'cpc_status',\n",
    "        'cpc_value',\n",
    "        'cpc_version_date',\n",
    "        'cpc_office_country',\n",
    "        'cpc_main_group',\n",
    "        'cpc_scheme_origin',\n",
    "        'cpc_section',\n",
    "        'cpc_subclass',\n",
    "        'cpc_subgroup',\n",
    "        'cpc_symbol_position',\n",
    "        'ipc_classification',\n",
    "        'invention_title',\n",
    "        'publication_country',\n",
    "        'publication_date',\n",
    "        'publication_number',\n",
    "        'publication_kind',\n",
    "        'application_series_code',\n",
    "        'inventors',\n",
    "        'applicants',\n",
    "        'source_file',  # Added missing column\n",
    "        'ingestion_date'  # Added missing column\n",
    "    ]\n",
    "    \n",
    "    # Select these columns from the original DataFrame\n",
    "    try:\n",
    "        print(f\"Number of selected columns: {len(selected_columns)}\")\n",
    "        print(f\"Number of new column names: {len(new_column_names)}\")\n",
    "        \n",
    "        # Make sure number of columns match\n",
    "        if len(selected_columns) != len(new_column_names):\n",
    "            print(f\"WARNING: Column count mismatch - using simpler approach\")\n",
    "            # Add a safer fallback that doesn't rely on exact column matching\n",
    "            silver_df = df\n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in df.columns:\n",
    "                    silver_df = silver_df.withColumnRenamed(old_col, new_col)\n",
    "        else:\n",
    "            selected_df = df.select(*selected_columns)\n",
    "            # Create the renamed DataFrame\n",
    "            silver_df = selected_df.toDF(*new_column_names)\n",
    "        \n",
    "        # Add ingestion metadata\n",
    "        silver_df = silver_df.withColumn(\"silver_ingestion_date\", current_timestamp())\n",
    "    except Exception as e:\n",
    "        print(f\"Error in transform operation: {str(e)}\")\n",
    "        # Fallback to simpler approach if column mapping fails\n",
    "        silver_df = df.withColumn(\"silver_ingestion_date\", current_timestamp())\n",
    "    \n",
    "    return silver_df, selected_columns\n",
    "\n",
    "def check_checkpoint_exists(spark, file_name, checkpoint_location):\n",
    "    \"\"\"Check if a checkpoint exists for the given file name\"\"\"\n",
    "    checkpoint_path = f\"{checkpoint_location}/{file_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Try to read the checkpoint file\n",
    "        dbutils.fs.ls(checkpoint_path)\n",
    "        # If we get here, the checkpoint exists\n",
    "        return True\n",
    "    except:\n",
    "        # Checkpoint doesn't exist\n",
    "        return False\n",
    "\n",
    "def create_checkpoint_file(spark, checkpoint_path, dir_name):\n",
    "    \"\"\"Create a checkpoint file with explicit schema to avoid type inference issues\"\"\"\n",
    "    try:\n",
    "        # Define schema explicitly\n",
    "        checkpoint_schema = StructType([\n",
    "            StructField(\"file_name\", StringType(), False),\n",
    "            StructField(\"processed_timestamp\", TimestampType(), False)\n",
    "        ])\n",
    "        \n",
    "        # Create a row with current timestamp\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Create DataFrame with explicit schema\n",
    "        checkpoint_df = spark.createDataFrame(\n",
    "            [(dir_name, current_time)],\n",
    "            schema=checkpoint_schema\n",
    "        )\n",
    "        \n",
    "        # Write checkpoint\n",
    "        checkpoint_df.write.format(\"delta\").mode(\"overwrite\").save(checkpoint_path)\n",
    "        print(f\"Created checkpoint at {checkpoint_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create checkpoint file: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Checkpoint error details: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def silver_layer_processing():\n",
    "    \"\"\"Process bronze Parquet files into silver Delta tables using incremental loading\"\"\"\n",
    "    print(\"Starting silver layer processing\")\n",
    "    \n",
    "    # Try to create a widget to control reprocessing (only works in interactive mode)\n",
    "    try:\n",
    "        dbutils.widgets.dropdown(\"force_reprocess\", \"false\", [\"true\", \"false\"], \"Force Reprocessing\")\n",
    "        force_reprocess = dbutils.widgets.get(\"force_reprocess\") == \"true\"\n",
    "    except:\n",
    "        # Default to incremental processing in job mode\n",
    "        force_reprocess = False\n",
    "    \n",
    "    print(f\"Force reprocessing mode: {force_reprocess}\")\n",
    "    \n",
    "    spark = initialize_spark()\n",
    "    \n",
    "    # Unity Catalog paths\n",
    "    bronze_path = \"/Volumes/nokia-assginment-catalog/bronze/raw_data\"\n",
    "    silver_path = \"/Volumes/nokia-assginment-catalog/silver\"\n",
    "    checkpoint_location = \"/Volumes/nokia-assginment-catalog/checkpoints/checkpoints_data/silver_autoloader/\"\n",
    "    \n",
    "    # Use a clear path for Delta output\n",
    "    delta_output_path = f\"{silver_path}/patent_data\"\n",
    "    \n",
    "    try:\n",
    "        # Check bronze directory to confirm files exist\n",
    "        print(f\"Checking bronze directory: {bronze_path}\")\n",
    "        try:\n",
    "            bronze_items = dbutils.fs.ls(bronze_path)\n",
    "            patent_dirs = [d for d in bronze_items if not d.name.endswith('.parquet')]\n",
    "            print(f\"Found {len(patent_dirs)} patent directories in bronze layer\")\n",
    "            for dir_item in patent_dirs[:5]:  # List first 5 directories\n",
    "                print(f\"  {dir_item.name}\")\n",
    "            \n",
    "            if len(patent_dirs) == 0:\n",
    "                print(\"ERROR: No patent directories found in bronze layer!\")\n",
    "                return False, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing bronze files: {str(e)}\")\n",
    "            return False, None\n",
    "        \n",
    "        # Handle checkpoint directory based on force_reprocess flag\n",
    "        if force_reprocess:\n",
    "            print(f\"Force reprocessing requested, clearing checkpoint location: {checkpoint_location}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(checkpoint_location, True)\n",
    "                print(\"Checkpoint directory cleared\")\n",
    "            except:\n",
    "                print(\"No checkpoint directory to clear\")\n",
    "            \n",
    "            dbutils.fs.mkdirs(checkpoint_location)\n",
    "            print(\"Created new checkpoint directory\")\n",
    "        else:\n",
    "            print(\"Using existing checkpoint directory for incremental processing\")\n",
    "            # Just ensure the directory exists\n",
    "            try:\n",
    "                dbutils.fs.ls(checkpoint_location)\n",
    "                print(\"Checkpoint directory exists\")\n",
    "            except:\n",
    "                dbutils.fs.mkdirs(checkpoint_location)\n",
    "                print(\"Created checkpoint directory\")\n",
    "        \n",
    "        # Handle output directory based on force_reprocess flag\n",
    "        if force_reprocess:\n",
    "            print(f\"Force reprocessing requested, clearing output directory: {delta_output_path}\")\n",
    "            try:\n",
    "                dbutils.fs.rm(delta_output_path, True)\n",
    "                print(\"Removed existing Delta directory\")\n",
    "            except:\n",
    "                print(\"No existing Delta directory to remove\")\n",
    "            \n",
    "            dbutils.fs.mkdirs(delta_output_path)\n",
    "            print(\"Created Delta output directory\")\n",
    "        else:\n",
    "            print(\"Using existing output directory for incremental files\")\n",
    "            # Just ensure the directory exists\n",
    "            try:\n",
    "                dbutils.fs.ls(delta_output_path)\n",
    "                print(\"Delta output directory exists\")\n",
    "            except:\n",
    "                dbutils.fs.mkdirs(delta_output_path)\n",
    "                print(\"Created Delta output directory\")\n",
    "        \n",
    "        # Process each patent directory incrementally\n",
    "        total_processed = 0\n",
    "        stats_df = None\n",
    "        \n",
    "        for dir_index, dir_item in enumerate(patent_dirs):\n",
    "            dir_path = dir_item.path\n",
    "            dir_name = dir_item.name\n",
    "            \n",
    "            # Use checkpoint location for tracking progress\n",
    "            file_checkpoint_path = f\"{checkpoint_location}/{dir_name}\"\n",
    "            \n",
    "            # Check if directory was already processed using checkpoint (instead of output directory)\n",
    "            if not force_reprocess and check_checkpoint_exists(spark, dir_name, checkpoint_location):\n",
    "                print(f\"Skipping already processed directory (checkpoint found): {dir_name}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing directory {dir_index+1}/{len(patent_dirs)}: {dir_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Read the Parquet files from bronze layer\n",
    "                print(f\"Reading Parquet files from {dir_path}\")\n",
    "                bronze_df = spark.read.parquet(dir_path)\n",
    "                \n",
    "                print(f\"Loaded {bronze_df.count()} records from bronze layer\")\n",
    "                \n",
    "                # Transform and rename fields\n",
    "                print(\"Transforming and renaming fields\")\n",
    "                silver_df, selected_columns = transform_and_rename_fields(bronze_df)\n",
    "                \n",
    "                # Write to Delta table\n",
    "                delta_table_path = f\"{delta_output_path}/{dir_name}\"\n",
    "                print(f\"Writing to Delta table at {delta_table_path}\")\n",
    "                \n",
    "                (silver_df.write\n",
    "                    .format(\"delta\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .option(\"overwriteSchema\", \"true\")\n",
    "                    .save(delta_table_path))\n",
    "                \n",
    "                # Create checkpoint file to mark successful processing with fixed schema\n",
    "                try:\n",
    "                    # Ensure checkpoint directory exists\n",
    "                    dbutils.fs.mkdirs(os.path.dirname(file_checkpoint_path))\n",
    "                    \n",
    "                    # Create checkpoint with proper schema\n",
    "                    create_checkpoint_file(spark, file_checkpoint_path, dir_name)\n",
    "                except Exception as checkpoint_error:\n",
    "                    print(f\"Warning: Could not create checkpoint file: {str(checkpoint_error)}\")\n",
    "                \n",
    "                # Get statistics on this batch\n",
    "                batch_stats_df, _ = analyze_field_occurrence(silver_df, max_depth=None)  # No depth limit\n",
    "                \n",
    "                # Store or merge statistics\n",
    "                if stats_df is None:\n",
    "                    stats_df = batch_stats_df\n",
    "                else:\n",
    "                    # In a real implementation, you'd merge statistics more intelligently\n",
    "                    stats_df = stats_df.union(batch_stats_df)\n",
    "                \n",
    "                total_processed += 1\n",
    "                print(f\"Successfully processed {dir_name} to Delta format\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {dir_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                print(f\"Exception traceback: {traceback.format_exc()}\")\n",
    "        \n",
    "        print(f\"Completed silver layer processing. Total directories processed: {total_processed}\")\n",
    "        \n",
    "        # Return success and the stats dataframe\n",
    "        return True, stats_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in silver layer processing: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Exception traceback: {traceback.format_exc()}\")\n",
    "        return False, None\n",
    "\n",
    "# Execute the function and capture the return values\n",
    "success, stats_df = silver_layer_processing()\n",
    "\n",
    "# Show statistics if available\n",
    "if success and stats_df is not None:\n",
    "    print(\"\\nField Occurrence Statistics:\")\n",
    "    stats_df.show(truncate=False)\n",
    "    \n",
    "    # Convert to pandas for more detailed analysis if needed\n",
    "    stats_pd = stats_df.toPandas()\n",
    "    print(f\"\\nTotal unique fields analyzed: {len(stats_pd)}\")\n",
    "    print(f\"Fields with >80% occurrence: {len(stats_pd[stats_pd['Occurrence (%)'] > 80])}\")\n",
    "    print(f\"Fields with <20% occurrence: {len(stats_pd[stats_pd['Occurrence (%)'] < 20])}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 41657801065516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver_patent_schema_normalization",
   "widgets": {
    "force_reprocess": {
     "currentValue": "false",
     "nuid": "a7011635-aae4-43d2-85bf-1df5aeea06db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Force Reprocessing",
      "name": "force_reprocess",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Force Reprocessing",
      "name": "force_reprocess",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
