{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c78d0df1-7e35-4149-a9e4-566ee5bfed99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `nokia-assginment-catalog`;\n",
    "-- drop schema patent_data cascade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f3af505-2f1a-4c11-b7d4-8b7217919c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to create a widget to control schema dropping\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"drop_patent_data_schema\", \"false\", [\"true\", \"false\"], \"Drop schema patent_data cascade\")\n",
    "    drop_patent_data_schema = dbutils.widgets.get(\"drop_patent_data_schema\") == \"true\"\n",
    "except:\n",
    "    # Default to not dropping schema in job mode\n",
    "    drop_patent_data_schema = False\n",
    "\n",
    "print(f\"Drop patent_data schema setting: {drop_patent_data_schema}\")\n",
    "\n",
    "# Execute SQL to drop schema if requested\n",
    "if drop_patent_data_schema:\n",
    "    try:\n",
    "        print(\"Dropping schema patent_data cascade...\")\n",
    "        spark.sql(\"DROP SCHEMA IF EXISTS patent_data CASCADE\")\n",
    "        print(\"Schema patent_data successfully dropped\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping schema: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa1367ad-10f4-4822-9ea2-57059bd89c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat_ws, array_join, from_unixtime, to_date\n",
    "from pyspark.sql.functions import regexp_replace, lpad, when, length, expr, explode_outer, coalesce, transform, flatten\n",
    "from pyspark.sql.functions import collect_list, struct, array, current_timestamp\n",
    "from pyspark.sql.types import StringType, TimestampType, StructType, StructField\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "import traceback\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def initialize_spark():\n",
    "    \"\"\"Initialize Spark session with Delta Lake support\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Patent Gold Layer Processor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def clean_date(col_obj):\n",
    "    \"\"\"Clean and format date strings from various formats to standard date\"\"\"\n",
    "    return to_date(regexp_replace(col_obj.cast(\"string\"), r\"[\\[\\]]\", \"\"), \"yyyyMMdd\")\n",
    "\n",
    "def format_class(class_col):\n",
    "    \"\"\"Ensure class is exactly 2 digits\"\"\"\n",
    "    return lpad(class_col.cast(\"string\"), 2, \"0\")\n",
    "\n",
    "def format_group(group_col):\n",
    "    \"\"\"Format group (1-4 digits)\"\"\"\n",
    "    return when(group_col.isNotNull(), group_col.cast(\"string\")).otherwise(lit(\"\"))\n",
    "\n",
    "def format_subgroup(subgroup_col):\n",
    "    \"\"\"Ensure subgroup has at least 2 digits\"\"\"\n",
    "    return when(length(subgroup_col.cast(\"string\")) < 2, \n",
    "                lpad(subgroup_col.cast(\"string\"), 2, \"0\")\n",
    "            ).otherwise(subgroup_col.cast(\"string\"))\n",
    "\n",
    "def format_patent_class(section_col, class_col, subclass_col, main_group_col, subgroup_col):\n",
    "    \"\"\"Create standardized classification code string\"\"\"\n",
    "    return concat_ws(\"\", \n",
    "        section_col,                                    \n",
    "        format_class(class_col),                      \n",
    "        subclass_col,                                 \n",
    "        format_group(main_group_col),                 \n",
    "        lit(\"/\"),                                            \n",
    "        format_subgroup(subgroup_col)                 \n",
    "    )\n",
    "\n",
    "def check_path_exists(path):\n",
    "    \"\"\"Check if a path exists and is accessible\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_delta_table(spark, path):\n",
    "    \"\"\"Check if a path is a valid Delta table\"\"\"\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_checkpoint_exists(spark, file_name, checkpoint_location):\n",
    "    \"\"\"Check if a checkpoint exists for the given file name\"\"\"\n",
    "    checkpoint_path = f\"{checkpoint_location}/{file_name}\"\n",
    "    try:\n",
    "        dbutils.fs.ls(checkpoint_path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_checkpoint_file(spark, checkpoint_path, dir_name):\n",
    "    \"\"\"Create a checkpoint file with explicit schema to avoid type inference issues\"\"\"\n",
    "    try:\n",
    "        checkpoint_schema = StructType([\n",
    "            StructField(\"file_name\", StringType(), False),\n",
    "            StructField(\"processed_timestamp\", TimestampType(), False)\n",
    "        ])\n",
    "        \n",
    "        current_time = datetime.now()\n",
    "        checkpoint_df = spark.createDataFrame(\n",
    "            [(dir_name, current_time)],\n",
    "            schema=checkpoint_schema\n",
    "        )\n",
    "        \n",
    "        checkpoint_df.write.format(\"delta\").mode(\"overwrite\").save(checkpoint_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create checkpoint file: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "def update_processing_metadata(spark, new_data_processed, database_name=\"patent_data\"):\n",
    "    \"\"\"\n",
    "    Update the processing metadata table that tracks if new data was processed\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        new_data_processed: Whether new data was processed\n",
    "        database_name: Database name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create database if it doesn't exist\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        \n",
    "        # Create metadata table if it doesn't exist\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {database_name}.processing_metadata (\n",
    "            processing_id STRING,\n",
    "            processing_timestamp TIMESTAMP,\n",
    "            new_data_processed BOOLEAN,\n",
    "            batch_count INT,\n",
    "            detail STRING\n",
    "        ) USING DELTA\n",
    "        \"\"\")\n",
    "        \n",
    "        # Generate a unique ID\n",
    "        import uuid\n",
    "        processing_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Get current batch count\n",
    "        try:\n",
    "            checkpoint_location = \"/Volumes/nokia-assginment-catalog/checkpoints/checkpoints_data/gold_autoloader/\"\n",
    "            if check_path_exists(checkpoint_location):\n",
    "                checkpoints = dbutils.fs.ls(checkpoint_location)\n",
    "                batch_count = len([c for c in checkpoints if not c.name.startswith('_')])\n",
    "            else:\n",
    "                batch_count = 0\n",
    "        except:\n",
    "            batch_count = -1  # Error determining batch count\n",
    "        \n",
    "        # Insert record into metadata table\n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO {database_name}.processing_metadata VALUES (\n",
    "            '{processing_id}',\n",
    "            current_timestamp(),\n",
    "            {str(new_data_processed).lower()},\n",
    "            {batch_count},\n",
    "            'Gold layer processing completed'\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"Updated processing metadata: new_data_processed={new_data_processed}, batch_count={batch_count}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not update processing metadata: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "def merge_or_write_delta(spark, df, output_path, batch_mode=\"append\", merge_keys=[\"publication_number\"]):\n",
    "    \"\"\"\n",
    "    Performs a proper upsert (update if exists, insert if not exists) operation \n",
    "    using the specified merge keys.\n",
    "    Falls back to regular write if delta table doesn't exist yet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the Delta table already exists and is valid\n",
    "        is_valid_delta = is_delta_table(spark, output_path) \n",
    "        \n",
    "        if is_valid_delta and batch_mode != \"overwrite\":\n",
    "            # If it exists and is valid, perform a merge operation (upsert)\n",
    "            print(f\"Valid Delta table exists at {output_path}, performing merge operation\")\n",
    "            \n",
    "            # Create a DeltaTable object for the existing table\n",
    "            delta_table = DeltaTable.forPath(spark, output_path)\n",
    "            \n",
    "            # Create the merge condition dynamically based on the merge keys\n",
    "            merge_condition = \" AND \".join([f\"target.{key} = source.{key}\" for key in merge_keys])\n",
    "            \n",
    "            # Perform the merge operation\n",
    "            delta_table.alias(\"target\").merge(\n",
    "                df.alias(\"source\"),\n",
    "                merge_condition\n",
    "            ).whenMatchedUpdateAll(\n",
    "            ).whenNotMatchedInsertAll(\n",
    "            ).execute()\n",
    "            \n",
    "            print(f\"Merge operation completed for {output_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            # If the table doesn't exist, isn't valid, or we're in overwrite mode, just write the data\n",
    "            print(f\"Writing data to {output_path} using mode {batch_mode}\")\n",
    "            df.write.format(\"delta\").mode(batch_mode).save(output_path)\n",
    "            print(f\"Write operation completed for {output_path}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in merge/write operation: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        # Fallback to regular write if anything goes wrong with merge\n",
    "        try:\n",
    "            print(f\"Falling back to regular write operation for {output_path}\")\n",
    "            df.write.format(\"delta\").mode(batch_mode).save(output_path)\n",
    "            print(f\"Fallback write operation completed for {output_path}\")\n",
    "            return True\n",
    "        except Exception as write_error:\n",
    "            print(f\"Error in fallback write operation: {str(write_error)}\")\n",
    "            print(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "def extract_claim_text(claims_df):\n",
    "    \"\"\"Extract claim text values recursively with added diagnostics and fallbacks\"\"\"\n",
    "    try:\n",
    "        print(f\"Starting claim text extraction for {claims_df.count()} claims\")\n",
    "        \n",
    "        # Show claim structure for debugging\n",
    "        print(\"Claim structure sample:\")\n",
    "        claims_df.select(\"publication_number\", \"claim\").limit(1).show(truncate=False)\n",
    "        \n",
    "        # Check if claim-text exists\n",
    "        has_claim_text = claims_df.select(col(\"claim.claim-text\").isNotNull().alias(\"has_claim_text\"))\n",
    "        has_claim_text_count = has_claim_text.filter(\"has_claim_text = true\").count()\n",
    "        print(f\"Records with claim-text: {has_claim_text_count}\")\n",
    "        \n",
    "        if has_claim_text_count == 0:\n",
    "            print(\"WARNING: No claim-text fields found, using fallback extraction\")\n",
    "            # Return simple fallback with just publication number\n",
    "            return claims_df.select(\n",
    "                \"publication_number\",\n",
    "                lit(None).alias(\"value_array\")\n",
    "            )\n",
    "        \n",
    "        # First level extraction\n",
    "        level1_df = claims_df.select(\n",
    "            \"publication_number\", \n",
    "            explode_outer(col(\"claim.claim-text\")).alias(\"claim_text_obj\")\n",
    "        )\n",
    "        \n",
    "        print(f\"First level extraction produced {level1_df.count()} records\")\n",
    "        \n",
    "        # Check if _VALUE exists in the data\n",
    "        has_value = level1_df.select(col(\"claim_text_obj._VALUE\").isNotNull().alias(\"has_value\"))\n",
    "        has_value_count = has_value.filter(\"has_value = true\").count()\n",
    "        print(f\"Records with _VALUE: {has_value_count}\")\n",
    "        \n",
    "        # Extract direct _VALUE arrays\n",
    "        level2_df = level1_df.select(\n",
    "            \"publication_number\",\n",
    "            col(\"claim_text_obj._VALUE\").alias(\"value_array\")\n",
    "        )\n",
    "        \n",
    "        # Check for nested claim-text\n",
    "        nested_claims_df = level1_df.filter(col(\"claim_text_obj.claim-text\").isNotNull())\n",
    "        nested_count = nested_claims_df.count()\n",
    "        print(f\"Records with nested claim-text: {nested_count}\")\n",
    "        \n",
    "        if nested_count > 0:\n",
    "            print(\"Processing nested claims\")\n",
    "            try:\n",
    "                nested_df = nested_claims_df.select(\n",
    "                    \"publication_number\",\n",
    "                    explode_outer(col(\"claim_text_obj.claim-text\")).alias(\"nested_claim_text\")\n",
    "                ).select(\n",
    "                    \"publication_number\",\n",
    "                    col(\"nested_claim_text._VALUE\").alias(\"value_array\")\n",
    "                )\n",
    "                \n",
    "                # Union direct and nested values\n",
    "                result_df = level2_df.union(nested_df)\n",
    "                print(f\"Final extracted records: {result_df.count()}\")\n",
    "                return result_df\n",
    "            except Exception as nested_error:\n",
    "                print(f\"Error processing nested claims: {str(nested_error)}\")\n",
    "                print(traceback.format_exc())\n",
    "                # Return just the direct values if nested processing fails\n",
    "                return level2_df\n",
    "        else:\n",
    "            return level2_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in claim text extraction: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        # Return a DataFrame with the same schema but empty values as fallback\n",
    "        return claims_df.select(\n",
    "            \"publication_number\",\n",
    "            lit(None).alias(\"value_array\")\n",
    "        )\n",
    "\n",
    "def process_patents_gold(renamed_df, output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process the patent gold entity with properly formatted CPC and additional data\"\"\"\n",
    "    try:\n",
    "        # Create the standard patent gold dataframe\n",
    "        patent_df = renamed_df.select(\n",
    "            # Basic patent information\n",
    "            col(\"publication_number\"),\n",
    "            col(\"invention_title\"),\n",
    "            array_join(col(\"abstract_text\"), \" \").alias(\"abstract\"),\n",
    "            array_join(col(\"description_text\"), \" \").alias(\"description\"),\n",
    "\n",
    "            # Publication reference\n",
    "            col(\"publication_country\"),\n",
    "            clean_date(col(\"publication_date\")).alias(\"publication_date\"),\n",
    "            col(\"publication_kind\"),\n",
    "            \n",
    "            # Application reference\n",
    "            col(\"application_country\"),\n",
    "            clean_date(col(\"application_date\")).alias(\"application_date\"),\n",
    "            col(\"application_number\"),\n",
    "            col(\"application_series_code\"),\n",
    "            \n",
    "            # CPC components individually for later analysis\n",
    "            col(\"cpc_section\").alias(\"cpc_section\"),\n",
    "            format_class(col(\"cpc_class\")).alias(\"cpc_class\"),\n",
    "            col(\"cpc_subclass\").alias(\"cpc_subclass\"),\n",
    "            format_group(col(\"cpc_main_group\")).alias(\"cpc_group\"),\n",
    "            format_subgroup(col(\"cpc_subgroup\")).alias(\"cpc_subgroup\"),\n",
    "            \n",
    "            # CPC metadata\n",
    "            clean_date(col(\"cpc_action_date\")).alias(\"cpc_action_date\"),\n",
    "            col(\"cpc_data_source\"),\n",
    "            col(\"cpc_status\"),\n",
    "            col(\"cpc_value\"),\n",
    "            clean_date(col(\"cpc_version_date\")).alias(\"cpc_version_date\"),\n",
    "            col(\"cpc_office_country\"),\n",
    "            col(\"cpc_scheme_origin\"),\n",
    "            col(\"cpc_symbol_position\"),\n",
    "            \n",
    "            # Full CPC code with proper structure (e.g., A01B33/00)\n",
    "            format_patent_class(\n",
    "                col(\"cpc_section\"),\n",
    "                col(\"cpc_class\"),\n",
    "                col(\"cpc_subclass\"),\n",
    "                col(\"cpc_main_group\"),\n",
    "                col(\"cpc_subgroup\")\n",
    "            ).alias(\"cpc_main\"),\n",
    "            \n",
    "            # Hierarchical CPC code for tiered analysis\n",
    "            concat_ws(\"\", \n",
    "                col(\"cpc_section\"),                                  \n",
    "                format_class(col(\"cpc_class\"))                      \n",
    "            ).alias(\"cpc_class_level\"),\n",
    "            \n",
    "            concat_ws(\"\", \n",
    "                col(\"cpc_section\"),                                  \n",
    "                format_class(col(\"cpc_class\")),                     \n",
    "                col(\"cpc_subclass\")                                 \n",
    "            ).alias(\"cpc_subclass_level\")\n",
    "        )\n",
    "        \n",
    "        # Add gold ingestion metadata\n",
    "        patent_df = patent_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        \n",
    "        # Use merge operation for upsert capability\n",
    "        spark = renamed_df.sparkSession\n",
    "        merge_or_write_delta(spark, patent_df, output_path, batch_mode)\n",
    "        \n",
    "        return patent_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing patents gold: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def process_ipc_gold(renamed_df, output_path, individual_output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process IPC classifications for gold layer with both grouped and individual records\"\"\"\n",
    "    try:\n",
    "        # Process IPC classifications first\n",
    "        ipc_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"ipc_classification\")).alias(\"ipc\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"ipc.section\").alias(\"ipc_section\"),\n",
    "            col(\"ipc.class\").alias(\"ipc_class\"),\n",
    "            col(\"ipc.subclass\").alias(\"ipc_subclass\"),\n",
    "            col(\"ipc.main-group\").alias(\"ipc_main_group\"),\n",
    "            col(\"ipc.subgroup\").alias(\"ipc_subgroup\"),\n",
    "            col(\"ipc.classification-value\").alias(\"ipc_value\"),\n",
    "            clean_date(col(\"ipc.action-date.date\")).alias(\"ipc_action_date\"),\n",
    "            col(\"ipc.classification-status\").alias(\"ipc_status\"),\n",
    "            col(\"ipc.classification-level\").alias(\"ipc_level\"),\n",
    "            col(\"ipc.classification-data-source\").alias(\"ipc_data_source\"),\n",
    "            col(\"ipc.generating-office.country\").alias(\"ipc_office_country\"),\n",
    "            clean_date(col(\"ipc.ipc-version-indicator.date\")).alias(\"ipc_version_date\"),\n",
    "            col(\"ipc.symbol-position\").alias(\"ipc_symbol_position\")\n",
    "        )\n",
    "\n",
    "        # Add formatted IPC code\n",
    "        ipc_df = ipc_df.withColumn(\n",
    "            \"ipc_code\",\n",
    "            format_patent_class(\n",
    "                col(\"ipc_section\"),\n",
    "                col(\"ipc_class\"),\n",
    "                col(\"ipc_subclass\"),\n",
    "                col(\"ipc_main_group\"),\n",
    "                col(\"ipc_subgroup\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Save individual IPC records\n",
    "        ipc_individual_df = ipc_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        spark = renamed_df.sparkSession\n",
    "        merge_or_write_delta(spark, ipc_individual_df, individual_output_path, batch_mode)\n",
    "\n",
    "        # Group IPC information by publication_number\n",
    "        ipc_grouped_df = ipc_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"ipc_code\").alias(\"ipc_codes\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"ipc_code\", \"ipc_section\", \"ipc_class\", \"ipc_subclass\",\n",
    "                    \"ipc_main_group\", \"ipc_subgroup\", \"ipc_value\", \"ipc_action_date\",\n",
    "                    \"ipc_status\", \"ipc_level\", \"ipc_data_source\"\n",
    "                )\n",
    "            ).alias(\"ipc_details\")\n",
    "        )\n",
    "        \n",
    "        # Add gold ingestion metadata\n",
    "        ipc_grouped_df = ipc_grouped_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        \n",
    "        # Use merge operation for upsert capability\n",
    "        merge_or_write_delta(spark, ipc_grouped_df, output_path, batch_mode)\n",
    "        \n",
    "        return ipc_grouped_df, ipc_individual_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing IPC gold: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def process_inventors_gold(renamed_df, output_path, individual_output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process inventors for gold layer with both grouped and individual records\"\"\"\n",
    "    try:\n",
    "        # Extract inventors\n",
    "        inventors_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"inventors\")).alias(\"inventor\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"inventor.addressbook.first-name\").alias(\"inventor_first_name\"),\n",
    "            col(\"inventor.addressbook.last-name\").alias(\"inventor_last_name\"),\n",
    "            concat_ws(\" \", \n",
    "                col(\"inventor.addressbook.first-name\"), \n",
    "                col(\"inventor.addressbook.last-name\")\n",
    "            ).alias(\"inventor_name\"),\n",
    "            col(\"inventor.addressbook.address.city\").alias(\"inventor_city\"),\n",
    "            col(\"inventor.addressbook.address.state\").alias(\"inventor_state\"),\n",
    "            col(\"inventor.addressbook.address.country\").alias(\"inventor_country\")\n",
    "        )\n",
    "\n",
    "        # Save individual inventor records\n",
    "        inventors_individual_df = inventors_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        spark = renamed_df.sparkSession\n",
    "        merge_or_write_delta(spark, inventors_individual_df, individual_output_path, batch_mode)\n",
    "\n",
    "        # Group inventors\n",
    "        inventors_grouped_df = inventors_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"inventor_name\").alias(\"inventor_names\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"inventor_name\", \"inventor_city\", \"inventor_state\", \"inventor_country\"\n",
    "                )\n",
    "            ).alias(\"inventor_details\")\n",
    "        )\n",
    "        \n",
    "        # Add gold ingestion metadata\n",
    "        inventors_grouped_df = inventors_grouped_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        \n",
    "        # Use merge operation for upsert capability\n",
    "        merge_or_write_delta(spark, inventors_grouped_df, output_path, batch_mode)\n",
    "        \n",
    "        return inventors_grouped_df, inventors_individual_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing inventors gold: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def process_applicants_gold(renamed_df, output_path, individual_output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process applicants for gold layer with both grouped and individual records\"\"\"\n",
    "    try:\n",
    "        # Extract applicants\n",
    "        applicants_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"applicants\")).alias(\"applicant\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"applicant.addressbook.first-name\").alias(\"applicant_first_name\"),\n",
    "            col(\"applicant.addressbook.last-name\").alias(\"applicant_last_name\"),\n",
    "            col(\"applicant.addressbook.orgname\").alias(\"applicant_orgname\"),\n",
    "            when(col(\"applicant.addressbook.orgname\").isNotNull(), \n",
    "                 col(\"applicant.addressbook.orgname\"))\n",
    "            .otherwise(\n",
    "                concat_ws(\" \", \n",
    "                    col(\"applicant.addressbook.first-name\"),\n",
    "                    col(\"applicant.addressbook.last-name\")\n",
    "                )\n",
    "            ).alias(\"applicant_name\"),\n",
    "            col(\"applicant.addressbook.address.city\").alias(\"applicant_city\"),\n",
    "            col(\"applicant.addressbook.address.state\").alias(\"applicant_state\"),\n",
    "            col(\"applicant.addressbook.address.country\").alias(\"applicant_country\")\n",
    "        )\n",
    "\n",
    "        # Save individual applicant records\n",
    "        applicants_individual_df = applicants_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        spark = renamed_df.sparkSession\n",
    "        merge_or_write_delta(spark, applicants_individual_df, individual_output_path, batch_mode)\n",
    "\n",
    "        # Group applicants\n",
    "        applicants_grouped_df = applicants_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"applicant_name\").alias(\"applicant_names\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"applicant_name\", \"applicant_city\", \"applicant_state\", \"applicant_country\"\n",
    "                )\n",
    "            ).alias(\"applicant_details\")\n",
    "        )\n",
    "        \n",
    "        # Add gold ingestion metadata\n",
    "        applicants_grouped_df = applicants_grouped_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        \n",
    "        # Use merge operation for upsert capability\n",
    "        merge_or_write_delta(spark, applicants_grouped_df, output_path, batch_mode)\n",
    "        \n",
    "        return applicants_grouped_df, applicants_individual_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing applicants gold: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def process_us_applicants_gold(renamed_df, output_path, individual_output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process US applicants for gold layer with both grouped and individual records\"\"\"\n",
    "    try:\n",
    "        # Extract US applicants\n",
    "        us_applicants_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"applicants\")).alias(\"applicant\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"applicant.addressbook.first-name\").alias(\"applicant_first_name\"),\n",
    "            col(\"applicant.addressbook.last-name\").alias(\"applicant_last_name\"),\n",
    "            col(\"applicant.addressbook.orgname\").alias(\"applicant_orgname\"),\n",
    "            when(col(\"applicant.addressbook.orgname\").isNotNull(), \n",
    "                 col(\"applicant.addressbook.orgname\"))\n",
    "            .otherwise(\n",
    "                concat_ws(\" \", \n",
    "                    col(\"applicant.addressbook.first-name\"),\n",
    "                    col(\"applicant.addressbook.last-name\")\n",
    "                )\n",
    "            ).alias(\"applicant_name\"),\n",
    "            col(\"applicant.addressbook.address.city\").alias(\"applicant_city\"),\n",
    "            col(\"applicant.addressbook.address.state\").alias(\"applicant_state\"),\n",
    "            col(\"applicant.addressbook.address.country\").alias(\"applicant_country\")\n",
    "        ).filter(col(\"applicant_country\") == \"US\")  # Filter for US applicants\n",
    "\n",
    "        # Save individual US applicant records\n",
    "        us_applicants_individual_df = us_applicants_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        spark = renamed_df.sparkSession\n",
    "        merge_or_write_delta(spark, us_applicants_individual_df, individual_output_path, batch_mode)\n",
    "\n",
    "        # Group US applicants\n",
    "        us_applicants_grouped_df = us_applicants_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"applicant_name\").alias(\"us_applicant_names\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"applicant_name\", \"applicant_city\", \"applicant_state\", \"applicant_country\"\n",
    "                )\n",
    "            ).alias(\"us_applicant_details\")\n",
    "        )\n",
    "        \n",
    "        # Add gold ingestion metadata\n",
    "        us_applicants_grouped_df = us_applicants_grouped_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        \n",
    "        # Use merge operation for upsert capability\n",
    "        merge_or_write_delta(spark, us_applicants_grouped_df, output_path, batch_mode)\n",
    "        \n",
    "        return us_applicants_grouped_df, us_applicants_individual_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing US applicants gold: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def process_claims_gold(renamed_df, output_path, individual_output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process claims for gold layer with individual claims as separate rows\"\"\"\n",
    "    try:\n",
    "        print(f\"Starting claims processing\")\n",
    "        spark = renamed_df.sparkSession\n",
    "        \n",
    "        # Check if claims field exists\n",
    "        claims_exist_count = renamed_df.filter(col(\"claims\").isNotNull()).count()\n",
    "        print(f\"Records with non-null claims: {claims_exist_count}\")\n",
    "        \n",
    "        if claims_exist_count == 0:\n",
    "            print(\"WARNING: No claims data found\")\n",
    "            return None, None\n",
    "        \n",
    "        # Extract claims\n",
    "        claims_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"claims\")).alias(\"claim\")\n",
    "        )\n",
    "        \n",
    "        print(f\"Extracted {claims_df.count()} claim rows from patents\")\n",
    "        \n",
    "        # Use the existing extract_claim_text function to get text values\n",
    "        claim_values_df = extract_claim_text(claims_df)\n",
    "        \n",
    "        # Process the extracted values\n",
    "        from pyspark.sql.window import Window\n",
    "        from pyspark.sql.functions import row_number, concat_ws\n",
    "        \n",
    "        # Group by publication_number and assign a claim number to each claim\n",
    "        # First convert the value arrays to text\n",
    "        df_with_text = claim_values_df.withColumn(\n",
    "            \"claim_text\",\n",
    "            when(col(\"value_array\").isNotNull(), \n",
    "                 array_join(col(\"value_array\"), \" \")\n",
    "            ).otherwise(lit(\"No claim text available\"))\n",
    "        )\n",
    "        \n",
    "        # Group by publication_number to get complete claims\n",
    "        claims_by_patent = df_with_text.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"claim_text\").alias(\"claim_texts\")\n",
    "        )\n",
    "        \n",
    "        # Explode to get one row per claim again\n",
    "        claims_exploded = claims_by_patent.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"claim_texts\")).alias(\"claim_text\")\n",
    "        )\n",
    "        \n",
    "        # Assign sequential numbers to each claim\n",
    "        window_spec = Window.partitionBy(\"publication_number\").orderBy(\"claim_text\")\n",
    "        \n",
    "        claims_individual_df = claims_exploded.withColumn(\n",
    "            # Assign sequential number to each claim within a patent\n",
    "            \"claim_number\", \n",
    "            row_number().over(window_spec)\n",
    "        ).withColumn(\n",
    "            # Create a unique ID combining publication number and claim number\n",
    "            \"claim_id\", \n",
    "            concat_ws(\"_\", col(\"publication_number\"), col(\"claim_number\").cast(\"string\"))\n",
    "        ).withColumn(\n",
    "            \"gold_ingestion_date\", \n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        # Show the schema to confirm both columns exist\n",
    "        print(\"Individual claims schema:\")\n",
    "        claims_individual_df.printSchema()\n",
    "        \n",
    "        # Write individual claims directly (one row per claim)\n",
    "        individual_count = claims_individual_df.count()\n",
    "        print(f\"Writing {individual_count} individual claim records to {individual_output_path}\")\n",
    "        claims_individual_df.write.format(\"delta\").mode(batch_mode).save(individual_output_path)\n",
    "        \n",
    "        # Create grouped claims (publication_number -> all claims text)\n",
    "        claims_grouped_df = claims_individual_df.groupBy(\"publication_number\").agg(\n",
    "            array_join(collect_list(\"claim_text\"), \" \").alias(\"all_claims_text\")\n",
    "        ).withColumn(\n",
    "            \"gold_ingestion_date\", \n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        # Write grouped claims directly\n",
    "        grouped_count = claims_grouped_df.count()\n",
    "        print(f\"Writing {grouped_count} grouped claim records to {output_path}\")\n",
    "        claims_grouped_df.write.format(\"delta\").mode(batch_mode).save(output_path)\n",
    "        \n",
    "        # Check if files were written\n",
    "        try:\n",
    "            ind_files = dbutils.fs.ls(individual_output_path)\n",
    "            grp_files = dbutils.fs.ls(output_path)\n",
    "            ind_data = [f for f in ind_files if not f.name.startswith('_')]\n",
    "            grp_data = [f for f in grp_files if not f.name.startswith('_')]\n",
    "            print(f\"Individual claims directory contains {len(ind_data)} data files\")\n",
    "            print(f\"Grouped claims directory contains {len(grp_data)} data files\")\n",
    "        except Exception as ls_error:\n",
    "            print(f\"Error checking output: {str(ls_error)}\")\n",
    "        \n",
    "        return claims_grouped_df, claims_individual_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in claims processing: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "def process_complete_patents_gold(renamed_df, output_path, batch_mode=\"append\"):\n",
    "    \"\"\"Process the complete patent dataframe with all joined components for gold layer\"\"\"\n",
    "    try:\n",
    "        # Start with patent processing (without adding gold_ingestion_date yet)\n",
    "        patent_df = renamed_df.select(\n",
    "            # Basic patent information\n",
    "            col(\"publication_number\"),\n",
    "            col(\"invention_title\"),\n",
    "            array_join(col(\"abstract_text\"), \" \").alias(\"abstract\"),\n",
    "            array_join(col(\"description_text\"), \" \").alias(\"description\"),\n",
    "\n",
    "            # Publication reference\n",
    "            col(\"publication_country\"),\n",
    "            clean_date(col(\"publication_date\")).alias(\"publication_date\"),\n",
    "            col(\"publication_kind\"),\n",
    "            \n",
    "            # Application reference\n",
    "            col(\"application_country\"),\n",
    "            clean_date(col(\"application_date\")).alias(\"application_date\"),\n",
    "            col(\"application_number\"),\n",
    "            col(\"application_series_code\"),\n",
    "            \n",
    "            # CPC components individually for later analysis\n",
    "            col(\"cpc_section\").alias(\"cpc_section\"),\n",
    "            format_class(col(\"cpc_class\")).alias(\"cpc_class\"),\n",
    "            col(\"cpc_subclass\").alias(\"cpc_subclass\"),\n",
    "            format_group(col(\"cpc_main_group\")).alias(\"cpc_group\"),\n",
    "            format_subgroup(col(\"cpc_subgroup\")).alias(\"cpc_subgroup\"),\n",
    "            \n",
    "            # CPC metadata\n",
    "            clean_date(col(\"cpc_action_date\")).alias(\"cpc_action_date\"),\n",
    "            col(\"cpc_data_source\"),\n",
    "            col(\"cpc_status\"),\n",
    "            col(\"cpc_value\"),\n",
    "            clean_date(col(\"cpc_version_date\")).alias(\"cpc_version_date\"),\n",
    "            col(\"cpc_office_country\"),\n",
    "            col(\"cpc_scheme_origin\"),\n",
    "            col(\"cpc_symbol_position\"),\n",
    "            \n",
    "            # Full CPC code with proper structure (e.g., A01B33/00)\n",
    "            format_patent_class(\n",
    "                col(\"cpc_section\"),\n",
    "                col(\"cpc_class\"),\n",
    "                col(\"cpc_subclass\"),\n",
    "                col(\"cpc_main_group\"),\n",
    "                col(\"cpc_subgroup\")\n",
    "            ).alias(\"cpc_main\"),\n",
    "            \n",
    "            # Hierarchical CPC code for tiered analysis\n",
    "            concat_ws(\"\", \n",
    "                col(\"cpc_section\"),                                  \n",
    "                format_class(col(\"cpc_class\"))                      \n",
    "            ).alias(\"cpc_class_level\"),\n",
    "            \n",
    "            concat_ws(\"\", \n",
    "                col(\"cpc_section\"),                                  \n",
    "                format_class(col(\"cpc_class\")),                     \n",
    "                col(\"cpc_subclass\")                                 \n",
    "            ).alias(\"cpc_subclass_level\")\n",
    "        )\n",
    "\n",
    "        # Process claims (without adding gold_ingestion_date yet)\n",
    "        claims_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"claims\")).alias(\"claim\")\n",
    "        )\n",
    "\n",
    "        claim_values_df = extract_claim_text(claims_df)\n",
    "        claim_values_df = claim_values_df.withColumn(\n",
    "            \"claim_text\", \n",
    "            when(col(\"value_array\").isNotNull(),\n",
    "                 array_join(col(\"value_array\"), \" \")\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "\n",
    "        claims_grouped_df = claim_values_df.groupBy(\"publication_number\").agg(\n",
    "            array_join(collect_list(\"claim_text\"), \" \").alias(\"all_claims_text\")\n",
    "        )\n",
    "        \n",
    "        # Process IPC (without adding gold_ingestion_date yet)\n",
    "        ipc_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"ipc_classification\")).alias(\"ipc\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"ipc.section\").alias(\"ipc_section\"),\n",
    "            col(\"ipc.class\").alias(\"ipc_class\"),\n",
    "            col(\"ipc.subclass\").alias(\"ipc_subclass\"),\n",
    "            col(\"ipc.main-group\").alias(\"ipc_main_group\"),\n",
    "            col(\"ipc.subgroup\").alias(\"ipc_subgroup\"),\n",
    "            col(\"ipc.classification-value\").alias(\"ipc_value\"),\n",
    "            clean_date(col(\"ipc.action-date.date\")).alias(\"ipc_action_date\"),\n",
    "            col(\"ipc.classification-status\").alias(\"ipc_status\"),\n",
    "            col(\"ipc.classification-level\").alias(\"ipc_level\"),\n",
    "            col(\"ipc.classification-data-source\").alias(\"ipc_data_source\"),\n",
    "            col(\"ipc.generating-office.country\").alias(\"ipc_office_country\"),\n",
    "            clean_date(col(\"ipc.ipc-version-indicator.date\")).alias(\"ipc_version_date\"),\n",
    "            col(\"ipc.symbol-position\").alias(\"ipc_symbol_position\")\n",
    "        )\n",
    "\n",
    "        ipc_df = ipc_df.withColumn(\n",
    "            \"ipc_code\",\n",
    "            format_patent_class(\n",
    "                col(\"ipc_section\"),\n",
    "                col(\"ipc_class\"),\n",
    "                col(\"ipc_subclass\"),\n",
    "                col(\"ipc_main_group\"),\n",
    "                col(\"ipc_subgroup\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ipc_grouped_df = ipc_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"ipc_code\").alias(\"ipc_codes\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"ipc_code\", \"ipc_section\", \"ipc_class\", \"ipc_subclass\",\n",
    "                    \"ipc_main_group\", \"ipc_subgroup\", \"ipc_value\", \"ipc_action_date\",\n",
    "                    \"ipc_status\", \"ipc_level\", \"ipc_data_source\"\n",
    "                )\n",
    "            ).alias(\"ipc_details\")\n",
    "        )\n",
    "        \n",
    "        # Process inventors (without adding gold_ingestion_date yet)\n",
    "        inventors_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"inventors\")).alias(\"inventor\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"inventor.addressbook.first-name\").alias(\"inventor_first_name\"),\n",
    "            col(\"inventor.addressbook.last-name\").alias(\"inventor_last_name\"),\n",
    "            concat_ws(\" \", \n",
    "                col(\"inventor.addressbook.first-name\"), \n",
    "                col(\"inventor.addressbook.last-name\")\n",
    "            ).alias(\"inventor_name\"),\n",
    "            col(\"inventor.addressbook.address.city\").alias(\"inventor_city\"),\n",
    "            col(\"inventor.addressbook.address.state\").alias(\"inventor_state\"),\n",
    "            col(\"inventor.addressbook.address.country\").alias(\"inventor_country\")\n",
    "        )\n",
    "\n",
    "        inventors_grouped_df = inventors_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"inventor_name\").alias(\"inventor_names\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"inventor_name\", \"inventor_city\", \"inventor_state\", \"inventor_country\"\n",
    "                )\n",
    "            ).alias(\"inventor_details\")\n",
    "        )\n",
    "        \n",
    "        # Process applicants (without adding gold_ingestion_date yet)\n",
    "        applicants_df = renamed_df.select(\n",
    "            col(\"publication_number\"),\n",
    "            explode_outer(col(\"applicants\")).alias(\"applicant\")\n",
    "        ).select(\n",
    "            col(\"publication_number\"),\n",
    "            col(\"applicant.addressbook.first-name\").alias(\"applicant_first_name\"),\n",
    "            col(\"applicant.addressbook.last-name\").alias(\"applicant_last_name\"),\n",
    "            col(\"applicant.addressbook.orgname\").alias(\"applicant_orgname\"),\n",
    "            when(col(\"applicant.addressbook.orgname\").isNotNull(), \n",
    "                 col(\"applicant.addressbook.orgname\"))\n",
    "            .otherwise(\n",
    "                concat_ws(\" \", \n",
    "                    col(\"applicant.addressbook.first-name\"),\n",
    "                    col(\"applicant.addressbook.last-name\")\n",
    "                )\n",
    "            ).alias(\"applicant_name\"),\n",
    "            col(\"applicant.addressbook.address.city\").alias(\"applicant_city\"),\n",
    "            col(\"applicant.addressbook.address.state\").alias(\"applicant_state\"),\n",
    "            col(\"applicant.addressbook.address.country\").alias(\"applicant_country\")\n",
    "        )\n",
    "\n",
    "        applicants_grouped_df = applicants_df.groupBy(\"publication_number\").agg(\n",
    "            collect_list(\"applicant_name\").alias(\"applicant_names\"),\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    \"applicant_name\", \"applicant_city\", \"applicant_state\", \"applicant_country\"\n",
    "                )\n",
    "            ).alias(\"applicant_details\")\n",
    "        )\n",
    "        \n",
    "        # Join all components\n",
    "        complete_patent_df = patent_df.join(\n",
    "            claims_grouped_df, on=\"publication_number\", how=\"left\"\n",
    "        ).join(\n",
    "            ipc_grouped_df, on=\"publication_number\", how=\"left\"\n",
    "        ).join(\n",
    "            inventors_grouped_df, on=\"publication_number\", how=\"left\"\n",
    "        ).join(\n",
    "            applicants_grouped_df, on=\"publication_number\", how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # Add gold ingestion metadata - only once after joining\n",
    "        complete_patent_df = complete_patent_df.withColumn(\"gold_ingestion_date\", current_timestamp())\n",
    "        \n",
    "        # Use merge operation for upsert capability\n",
    "        spark = renamed_df.sparkSession\n",
    "        merge_or_write_delta(spark, complete_patent_df, output_path, batch_mode)\n",
    "        \n",
    "        return complete_patent_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing complete patents gold: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def gold_layer_processing():\n",
    "    \"\"\"Process silver Delta files into gold Delta tables for each entity\"\"\"\n",
    "    try:\n",
    "        # Try to create a widget to control reprocessing (only works in interactive mode)\n",
    "        try:\n",
    "            dbutils.widgets.dropdown(\"force_reprocess\", \"false\", [\"true\", \"false\"], \"Force Reprocessing\")\n",
    "            force_reprocess = dbutils.widgets.get(\"force_reprocess\") == \"true\"\n",
    "        except:\n",
    "            # Default to incremental processing in job mode\n",
    "            force_reprocess = False\n",
    "        \n",
    "        print(f\"Force reprocessing mode: {force_reprocess}\")\n",
    "        \n",
    "        spark = initialize_spark()\n",
    "        \n",
    "        # Fixed paths - silver data is directly in patent_data\n",
    "        silver_path = \"/Volumes/nokia-assginment-catalog/silver/patent_data\"\n",
    "        gold_path = \"/Volumes/nokia-assginment-catalog/gold\"\n",
    "        checkpoint_location = \"/Volumes/nokia-assginment-catalog/checkpoints/checkpoints_data/gold_autoloader/\"\n",
    "        \n",
    "        # Define gold entity paths - add paths for ungrouped data\n",
    "        entity_paths = {\n",
    "            \"patents\": f\"{gold_path}/patents\",\n",
    "            \"ipc_classifications\": f\"{gold_path}/ipc_classifications\", \n",
    "            \"ipc_individual\": f\"{gold_path}/ipc_individual\",  # Added path for individual IPCs\n",
    "            \"inventors\": f\"{gold_path}/inventors\",\n",
    "            \"inventors_individual\": f\"{gold_path}/inventors_individual\",  # Added path for individual inventors\n",
    "            \"applicants\": f\"{gold_path}/applicants\",\n",
    "            \"applicants_individual\": f\"{gold_path}/applicants_individual\",  # Added path for individual applicants\n",
    "            \"us_applicants\": f\"{gold_path}/us_applicants\",\n",
    "            \"us_applicants_individual\": f\"{gold_path}/us_applicants_individual\",  # Added path for individual US applicants\n",
    "            \"claims\": f\"{gold_path}/claims\",\n",
    "            \"claims_individual\": f\"{gold_path}/claims_individual\",  # Added path for individual claims\n",
    "            \"complete_patents\": f\"{gold_path}/complete_patents\"\n",
    "        }\n",
    "        \n",
    "        # Check silver patent_data directory\n",
    "        try:\n",
    "            if not check_path_exists(silver_path):\n",
    "                return False, f\"Silver patent_data path does not exist: {silver_path}\"\n",
    "                \n",
    "            # List batch directories in the patent_data folder\n",
    "            silver_items = dbutils.fs.ls(silver_path)\n",
    "            \n",
    "            # Get the batch directories (batch1, batch2, etc.)\n",
    "            batch_dirs = [d for d in silver_items if d.isDir() and not d.name.startswith('_')]\n",
    "            \n",
    "            if len(batch_dirs) == 0:\n",
    "                return False, \"No batch directories found in silver/patent_data\"\n",
    "                \n",
    "            # We'll process each batch directory directly\n",
    "            delta_dirs = batch_dirs\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, f\"Error listing silver patent_data files: {str(e)}\"\n",
    "        \n",
    "        # Handle checkpoint directory based on force_reprocess flag\n",
    "        if force_reprocess:\n",
    "            try:\n",
    "                dbutils.fs.rm(checkpoint_location, True)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                dbutils.fs.mkdirs(checkpoint_location)\n",
    "            except Exception as e:\n",
    "                return False, f\"Error creating checkpoint directory: {str(e)}\"\n",
    "        else:\n",
    "            # Just ensure the directory exists\n",
    "            try:\n",
    "                if not check_path_exists(checkpoint_location):\n",
    "                    dbutils.fs.mkdirs(checkpoint_location)\n",
    "            except Exception as e:\n",
    "                return False, f\"Error checking/creating checkpoint directory: {str(e)}\"\n",
    "        \n",
    "        # Handle output directories based on force_reprocess flag\n",
    "        write_mode = \"overwrite\" if force_reprocess else \"append\"\n",
    "        \n",
    "        for entity, path in entity_paths.items():\n",
    "            if force_reprocess:\n",
    "                try:\n",
    "                    dbutils.fs.rm(path, True)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                                        dbutils.fs.mkdirs(path)\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error creating output directory for {entity}: {str(e)}\"\n",
    "            else:\n",
    "                # Just ensure the directory exists\n",
    "                try:\n",
    "                    if not check_path_exists(path):\n",
    "                        dbutils.fs.mkdirs(path)\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error with output directory for {entity}: {str(e)}\"\n",
    "        \n",
    "        # Process each batch directory incrementally\n",
    "        total_processed = 0\n",
    "        total_errors = 0\n",
    "        new_data_processed = False  # Track if any new data was processed\n",
    "        \n",
    "        for dir_index, dir_item in enumerate(delta_dirs):\n",
    "            dir_path = dir_item.path\n",
    "            dir_name = dir_item.name.rstrip('/')  # Remove trailing slash if present\n",
    "            \n",
    "            # Use checkpoint location for tracking progress\n",
    "            file_checkpoint_path = f\"{checkpoint_location}/{dir_name}\"\n",
    "            \n",
    "            print(f\"Processing batch directory {dir_index+1}/{len(delta_dirs)}: {dir_name}\")\n",
    "            \n",
    "            # Check if directory was already processed using checkpoint\n",
    "            if not force_reprocess and check_checkpoint_exists(spark, dir_name, checkpoint_location):\n",
    "                print(f\"Skipping already processed batch directory (checkpoint found): {dir_name}\")\n",
    "                continue\n",
    "            \n",
    "            # If we get here, we're processing new data\n",
    "            new_data_processed = True\n",
    "            \n",
    "            try:\n",
    "                # Read the Delta files from silver layer\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    silver_df = spark.read.format(\"delta\").load(dir_path)\n",
    "                    record_count = silver_df.count()\n",
    "                    end_time = time.time()\n",
    "                    print(f\"Loaded {record_count} records from batch directory in {end_time - start_time:.2f} seconds\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading Delta files from batch directory {dir_path}: {str(e)}\")\n",
    "                    print(traceback.format_exc())\n",
    "                    total_errors += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process for each gold entity\n",
    "                try:\n",
    "                    # Use append mode for all but the first batch if it's first run \n",
    "                    # or use overwrite for all if force_reprocess is True\n",
    "                    current_batch_mode = write_mode\n",
    "                    \n",
    "                    # Process patents gold entity\n",
    "                    process_patents_gold(silver_df, entity_paths[\"patents\"], current_batch_mode)\n",
    "                    \n",
    "                    # Process IPC classifications gold entity - both grouped and individual\n",
    "                    process_ipc_gold(silver_df, entity_paths[\"ipc_classifications\"], \n",
    "                                     entity_paths[\"ipc_individual\"], current_batch_mode)\n",
    "                    \n",
    "                    # Process inventors gold entity - both grouped and individual\n",
    "                    process_inventors_gold(silver_df, entity_paths[\"inventors\"], \n",
    "                                          entity_paths[\"inventors_individual\"], current_batch_mode)\n",
    "                    \n",
    "                    # Process applicants gold entity - both grouped and individual\n",
    "                    process_applicants_gold(silver_df, entity_paths[\"applicants\"], \n",
    "                                           entity_paths[\"applicants_individual\"], current_batch_mode)\n",
    "                    \n",
    "                    # Process US applicants gold entity - both grouped and individual\n",
    "                    process_us_applicants_gold(silver_df, entity_paths[\"us_applicants\"], \n",
    "                                              entity_paths[\"us_applicants_individual\"], current_batch_mode)\n",
    "                    \n",
    "                    # Process claims gold entity - both grouped and individual\n",
    "                    process_claims_gold(silver_df, entity_paths[\"claims\"], \n",
    "                                       entity_paths[\"claims_individual\"], current_batch_mode)\n",
    "                    \n",
    "                    # Process complete patents gold entity\n",
    "                    process_complete_patents_gold(silver_df, entity_paths[\"complete_patents\"], current_batch_mode)\n",
    "                    \n",
    "                    # If first batch was processed successfully, switch to append mode for remaining batches\n",
    "                    if write_mode == \"overwrite\":\n",
    "                        write_mode = \"append\"\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during entity processing for {dir_name}: {str(e)}\")\n",
    "                    print(traceback.format_exc())\n",
    "                    total_errors += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create checkpoint file to mark successful processing with fixed schema\n",
    "                try:\n",
    "                    # Ensure checkpoint directory exists\n",
    "                    parent_dir = os.path.dirname(file_checkpoint_path)\n",
    "                    if not check_path_exists(parent_dir):\n",
    "                        dbutils.fs.mkdirs(parent_dir)\n",
    "                    \n",
    "                    # Create checkpoint with proper schema\n",
    "                    create_checkpoint_file(spark, file_checkpoint_path, dir_name)\n",
    "                except Exception as checkpoint_error:\n",
    "                    print(f\"Warning: Could not create checkpoint file: {str(checkpoint_error)}\")\n",
    "                    print(traceback.format_exc())\n",
    "                \n",
    "                total_processed += 1\n",
    "                print(f\"Successfully processed {dir_name} to all gold entities\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {dir_name}: {str(e)}\")\n",
    "                print(traceback.format_exc())\n",
    "                total_errors += 1\n",
    "        \n",
    "        print(f\"Completed gold layer processing. Directories processed: {total_processed}, Errors: {total_errors}\")\n",
    "        \n",
    "        # Update processing metadata to let the next step know if new data was processed\n",
    "        update_processing_metadata(spark, new_data_processed)\n",
    "        \n",
    "        # Store new_data_processed in a special table to communicate with the next step\n",
    "        try:\n",
    "            processing_status_path = \"/Volumes/nokia-assginment-catalog/processing_status\"\n",
    "            if not check_path_exists(processing_status_path):\n",
    "                dbutils.fs.mkdirs(processing_status_path)\n",
    "                \n",
    "            # Create a simple DataFrame with the processing status\n",
    "            status_df = spark.createDataFrame([(new_data_processed,)], [\"new_data_processed\"])\n",
    "            status_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{processing_status_path}/status\")\n",
    "            \n",
    "            print(f\"Stored processing status: new_data_processed={new_data_processed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not store processing status: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "        # Return success with stats including whether new data was processed\n",
    "        return True, {\"processed\": total_processed, \"errors\": total_errors, \"new_data_processed\": new_data_processed}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in gold layer processing: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return False, str(e)\n",
    "\n",
    "# Main function for running the processing standalone\n",
    "def main():\n",
    "    success, result = gold_layer_processing()\n",
    "    if success:\n",
    "        if isinstance(result, dict):\n",
    "            print(f\"Gold layer processing completed successfully.\")\n",
    "            print(f\"Processed: {result.get('processed', 0)} batches\")\n",
    "            print(f\"Errors: {result.get('errors', 0)}\")\n",
    "            print(f\"New data processed: {result.get('new_data_processed', False)}\")\n",
    "            \n",
    "            # Also return this as a proper JSON result\n",
    "            import json\n",
    "            result_json = json.dumps(result)\n",
    "            dbutils.notebook.exit(result_json)\n",
    "        else:\n",
    "            print(f\"Gold layer processing completed with message: {result}\")\n",
    "            dbutils.notebook.exit(json.dumps({\"success\": True, \"message\": str(result), \"new_data_processed\": False}))\n",
    "    else:\n",
    "        print(f\"Gold layer processing failed: {result}\")\n",
    "        dbutils.notebook.exit(json.dumps({\"success\": False, \"error\": str(result)}))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1357385789650411,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_to_gold_extracting_different_data",
   "widgets": {
    "force_reprocess": {
     "currentValue": "false",
     "nuid": "0cefd64c-334b-4187-946e-8871a00d6e26",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Force Reprocessing",
      "name": "force_reprocess",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Force Reprocessing",
      "name": "force_reprocess",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
